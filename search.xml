<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[前端json传参，后端spring如何承接]]></title>
    <url>%2F2020%2F01%2F11%2Fjson4spring%2F</url>
    <content type="text"><![CDATA[第一篇公众号就讲个很简单的问题吧。前些天重构中前端传过来的参数突然就接受不到了。究其根本就是前端同学统一换了参数的提交格式，从 application/x-www-form-urlencoded 更换成 application/json。一句话解释本文就是 application/json 需要搭配 @RequestBody 使用。 一点点唠叨一直在想第一篇文章应该写什么，写点最近踩得坑还是最近学的东西还是工作内容，生活感悟，一时间无从下手。同时距离上一篇博客过去了已经 8 个月了，上一篇还是在从老东家走了之后的总结和一些感想。现在想来自己还是不能跳出这个循环，被社会毒打，要奋发图强，然后再回归正常生活，再变成一个懒鬼。但是被毒打之后还是会留给你一点点的启发的，比如开始点源码的技能树，也比如更加有一些危机感。做新手程序员，最好的成长之路还是大量的写代码，大量的阅读业务代码，不懂就问问同事。努力承担更多的责任，干更多事情，必然会成长的。这叫干好自己的工作，理解自己的业务。然后再开始思考学点新东西。如果不知道学什么就从 JDK 源代码开始吧，然后学习缓存 Redis，再来一个中间件 kafka。如果这几个东西都学会怎么使用了，运行机制是怎样的，再看看源码是怎么实现的。这就比很多人都强了，也不着急一步一步来吧。 正文第一篇公众号就讲个很简单的问题吧。前些天重构中前端传过来的参数突然就接受不到了。究其根本就是前端同学统一换了参数的提交格式，从 application/x-www-form-urlencoded 更换成 application/json。一句话解释本文就是 application/json 需要搭配 @RequestBody 使用。前端 json 传参，后端 spring 如何承接前端使用 application/json 传参，分两部分 post 请求和 get 请求，因为这两个不太一样。 POST针对 post 请求，后端必须使用@RequestBody 修饰你自定义的实体。修饰自定义的一个实例来接受：例如 BaseDTO，即使只有一个字段也必须写成一个自定义对象；spring 才能将前端传来的参数转化进我们的对象。这儿强调一点，包装类例如：Long Integer 也必须再包一层，就是把这些包装类放在一个自定义实体中。12345 @RequestMapping(&quot;/demo/c&quot;) public String cDemo(@RequestBody Demo demo) &#123; return JSON.toJSONString(demo); &#125;备注：post请求，参数放在Request body中 修饰一个 MAP&lt;String, Object&gt;，这样也能正确接收到前端传来的参数，但是使用起来还要通过字段名来获取，比较麻烦。但是，有一些场景很好用，例如不确定前端要传几个参数时，这种写法就派上了用场，不论前端传多少参数，统统接收就好。 12345 @RequestMapping(&quot;/demo/a&quot;) public String aDemo(@RequestBody Map&lt;String, Object&gt; map) &#123; return JSON.toJSONString(map); &#125;备注：post请求，参数放在Request body中 GET 针对 get 请求，一般来说 get 请求会将参数拼接在 URL 后面，这种直接用实体就能接受，@RequestParam 用也可以，不用也可以，只有一个字段，直接用包装类接受也可以 1234567891011@RequestMapping(&quot;/demo/f&quot;)public String fDemo(@RequestParam Long demo) &#123; return JSON.toJSONString(demo);&#125;@RequestMapping(&quot;/demo/g&quot;)public String gDemo(Long demo) &#123; return JSON.toJSONString(demo);&#125;备注：GET请求，参数放在Request Params中 但是也有同学不走寻常路，他使用的是 get 请求，但是把参数放在 requestBody 里面。这就必须要跟 POST 请求一样处理了，加@RequestBody 修饰自定义实体，包装类也需要再使用自定义对象包一层。 12345 @RequestMapping(&quot;/demo/c&quot;) public String cDemo(@RequestBody Demo demo) &#123; return JSON.toJSONString(demo); &#125;备注：GET请求，参数放在Request body中 所有代码可以放在自己的 SpringBoot 项目中，然后拿 postman 测试一下。1234567891011121314151617181920212223242526272829303132333435363738394041424344@RestControllerpublic class TestDemo &#123; @RequestMapping(&quot;/demo/a&quot;) public String aDemo(@RequestBody Map&lt;String, Object&gt; map) &#123; return JSON.toJSONString(map); &#125; @RequestMapping(&quot;/demo/b&quot;) public String bDemo(Map&lt;String, Object&gt; map) &#123; return JSON.toJSONString(map); &#125; @RequestMapping(&quot;/demo/c&quot;) public String cDemo(@RequestBody Demo demo) &#123; return JSON.toJSONString(demo); &#125; @RequestMapping(&quot;/demo/d&quot;) public String dDemo(Demo demo) &#123; return JSON.toJSONString(demo); &#125; @RequestMapping(&quot;/demo/e&quot;) public String eDemo(@RequestBody Long demo) &#123; return JSON.toJSONString(demo); &#125; @RequestMapping(&quot;/demo/f&quot;) public String fDemo(@RequestParam Long demo) &#123; return JSON.toJSONString(demo); &#125; @RequestMapping(&quot;/demo/g&quot;) public String gDemo(Long demo) &#123; return JSON.toJSONString(demo); &#125;&#125;@Datapublic class Demo &#123; private String demoName; private Integer num;&#125; 给自己挖坑下一篇是 AQS，力争写一篇看完就可以入门 AQS 源码的文章，并且让读者有冲动自己上手阅读。奥利给！！！公众号一起交流：]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在RedHat5.8上安装jira]]></title>
    <url>%2F2018%2F12%2F12%2Fjira%2F</url>
    <content type="text"><![CDATA[在某个抓耳挠腮写不出代码的傍晚，老大那个时常是灰色的头像开始疯狂跳动，正好处于转正前夕。我有点紧张了，点开了对话框。“XX，下周你看一下JIRA，搭建个平台出来，如果需要Linux机器的话，找我要一下。”“想了想，打了好几个字，然后删除，在对话框输入’好的‘，Enter”言归正传，聊一下怎么搭建这个平台，分享一些我的低级失误，大家就不要再犯了，不要再折腾自己了，虽然我知道你还得折腾，谁让咱们都是不信命的程序员呢！ 确定你要安装的JIRA产品你需要知道的事情是JIRA现在指的是一个产品组了，不是指某个具体的产品，具体如下图：啥也不多说了，我们要的就是JIRA Software 环境介绍官方推荐： jdk1.8 Mysql 5.5 5.6 本次搭建中我使用的： RedHat5.8 123456[root@djt_36_149 ~]# lsb_release -aLSB Version: :core-4.0-amd64:core-4.0-ia32:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-ia32:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-ia32:printing-4.0-noarchDistributor ID: RedHatEnterpriseServerDescription: Red Hat Enterprise Linux Server release 5.8 (Tikanga)Release: 5.8Codename: Tikanga jdk1.8.0_191 1234[root@djt_36_149 ~]# java -versionjava version &quot;1.8.0_191&quot;Java(TM) SE Runtime Environment (build 1.8.0_191-b12)Java HotSpot(TM) Server VM (build 25.191-b12, mixed mode) Mysql 123[root@djt_36_149 jiraPackage]# rpm -qa | grep -i mysqlMySQL-client-5.6.23-1.rhel5MySQL-server-5.6.23-1.rhel5 JIRA Softwareatlassian-jira-software-7.3.8-x64.bin 安装包提供https://pan.baidu.com/s/1-gT1s93KZfgO59U1mdfYPw 提取码: 8h3c Mysql安装MySQL的安装推荐一篇文章：https://www.cnblogs.com/rusking/p/4422986.html按照上面文章做没有问题；亲测但大致记录一下我安装时的一些细节： 检查老版本并卸载 看看Linux机器有没有Mysql 123[root@rhel204 /]# rpm -qa | grep -i mysqlMySQL-server-advanced-5.6.23-1.rhel5MySQL-client-advanced-5.6.23-1.rhel5 假设有，如上，则卸载 1[root@rhel204 /]# rpm -ev MySQL-server-advanced-5.6.23-1.rhel5 MySQL-client-advanced-5.6.23-1.rhel5 删除残余文件其实并不是所有含有mysql的文件都要删，把下面列出来的这几个删除就可以了； 123456[root@rhel201 mysql]# find / -name mysql* 找到所有的mysql目录，并删除。 rm -rf /usr/share/mysqlrm -rf /var/lib/mysqlrm -rf /usr/lib64/mysqlrm -rf /var/lib/mysqlrm -rf /etc/my.cnf.d 安装MySQL并创建需要的数据库 服务端和客户端都需要12345678[root@rhel204 MySQL 5.6.23-RMP]# rpm -ivh MySQL-server-advanced-5.6.23-1.rhel5.x86_64.rpm Preparing... ########################################### [100%]1:MySQL-server-advanced ########################################### [100%]warning: user mysql does not exist - using rootwarning: group mysql does not exist - using root[root@rhel204 MySQL 5.6.23-RMP for oraclelinux or rhel5-x86-64V74393-01]# rpm -ivh MySQL-client-advanced-5.6.23-1.rhel5.x86_64.rpm Preparing... ########################################### [100%]1:MySQL-client-advanced ########################################### [100%] 安装完成有个提示：You will find that password in ‘/root/.mysql_secret’这个目录中有第一次登录需要的密码；要记一下。 启动MySQL1234[root@rhel204 MySQL 5.6.23-RMP]# /etc/init.d/mysql startStarting MySQL........[ OK ][root@rhel204 MySQL 5.6.23-RMP]# /etc/init.d/mysql statusMySQL running (13003)[ OK ] 登录MySQL 登录mysql -u root -p 粘贴前文中那个目录下的初始密码； 设置密码mysqladmin -uroot -p旧密码 password 新密码最好手动输入不要粘贴，有些不能识别； 创建创建数据库jira并为其赋权 创建用户 12create user &apos;jira&apos;@&apos;%&apos; identified by &apos;123456&apos;;flush privileges; 创建数据库 1create databases jiradb character set utf8 collate utf8_bin; 为用户在这个数据库上赋予所有权限 12grant all privileges on jiradb.* to jira@&apos;%&apos; identified by &apos;123456&apos;flush privileges; 至此数据库准备完成。数据库这一块，如果操作没有得到预期的结果，就查询你自己的具体问题吧； JDK安装预配置这一点就不在介绍了，网上实在太多。123export JAVA_HOME=/usr/local/jdk1.8.0_191export JRE_HOME=$JAVA_HOME/jreexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar JIRA Software 安装配置 这个请按照烂泥先生的流程来做，大佬还提供比较新的版本的破解:https://www.ilanni.com/?p=12119 如果不顺利也可以在参考另一篇：http://www.yfshare.vip/2017/05/09/%E9%83%A8%E7%BD%B2JIRA-7-2-2-for-Linux/ JIRA使用参考https://www.jianshu.com/p/145b5c33f7d0https://www.jianshu.com/p/975385878cdehttp://www.confluence.cn/pages/viewpage.action?pageId=1671211 后记想来也是惭愧从接到任务到完成这个部署以及写完这个文章整整两天半花了出去；起初在RedHat5.8上安装Mysql5.6，个人感觉是真是老牛拉新车；厌烦，所以打算在本地mac上搭建一下，但我的Mysql数据库是8.0.11，能连接成功(补充一句，mysql-connect的jar包请使用5.1.44),但是在部署JIRA时，初始化有问题，官方不支持；转去RedHat5.8，好不容易弄好了数据库却又安装了一个 JIRA core ,这玩意又少功能；删除，重来。终于成功：放个图高兴一些； 文章如有错误，欢迎指正]]></content>
      <categories>
        <category>JIRA</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>Jira</tag>
        <tag>Jdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Spring中创建切面，使用AspectJ]]></title>
    <url>%2F2018%2F11%2F19%2FaopAspectJ%2F</url>
    <content type="text"><![CDATA[看了网上一些AspectJ的例子，大多一塌糊涂。说完这句话有点慌张，如果后续在学习中发现是我错了，再来打脸也不迟。说说我的理解，目前我所学习到的实现AOP(切面)的方式大致可以分为两类，SpringAOP与AspectJ.关于SpringAOP的实现前两篇文章已经写过小例子了，欢迎查看。这篇用AspectJ实现AOP的小例子。代码配置都很细节，新手记录文。 定义一个表演方法 定义接口 12345package aopAspectJ;public interface Performance &#123; String perform();&#125; 实现具体方法 12345678910111213package aopAspectJ;public class PerformanceImpl implements Performance &#123; @Override public String perform() &#123; try &#123; System.out.println(&quot;演出～～～&quot;); &#125; catch (Exception e) &#123; throw e; &#125; return new String(&quot;嗒嗒嗒&quot;); &#125;&#125; 定义一个切面，实现各种通知1234567891011121314151617181920212223242526package aopAspectJ;public aspect Audience &#123; pointcut perFormance(): execution(* aopAspectJ.Performance.perform(..)); before(): perFormance()&#123; System.out.println(&quot;表演前：手机静音！&quot;); &#125; before(): perFormance()&#123; System.out.println(&quot;表演前：请坐！&quot;); &#125; after(): perFormance()&#123; System.out.println(&quot;表演后：鼓掌！！！&quot;); &#125; after()returning(String str): perFormance()&#123; System.out.println(&quot;调用成功，表演很精彩，鼓掌！！！&quot;); &#125; after()throwing(Exception e): perFormance()&#123; System.out.println(&quot;调用失败，表演失败，退票！！！&quot;); &#125;&#125; 需要一些特殊的配置运行时的上下文:SpringAopAspectJ.xml123456&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;performance&quot; class=&quot;aopAspectJ.PerformanceImpl&quot; /&gt;&lt;/beans&gt; pom.xml要添加的依赖1234567891011121314151617181920212223&lt;!--learnAOP--&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.8.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.2.4&lt;/version&gt;&lt;/dependency&gt;&lt;!--AspectJ--&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjrt&lt;/artifactId&gt; &lt;version&gt;1.8.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjtools&lt;/artifactId&gt; &lt;version&gt;1.8.6&lt;/version&gt; &lt;/dependency&gt; 设置IDEA的编译方式 将编译方式修改为Ajc 在你的本地仓库中找到aspectjtools-1.8.6.jar这个包，路径配置：Path to Ajc compiler 单元测试 代码： 12345678910111213141516171819package aopAspectJ;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:SpringAopAspectJ.xml&quot;)public class PerformanceImplTest &#123; @Autowired private Performance performance; @Test public void perform() &#123; performance.perform(); &#125;&#125; 结果： 12345表演前：手机静音！表演前：请坐！演出～～～表演后：鼓掌！！！调用成功，表演很精彩，鼓掌！！！ 后记我的理解关于AOP的小例子写了三个，关于切面，切点，通知的定义，这个网上已经很多了不赘述。希望通过三个小例子，可以为你理清一点头绪，确定一下几点： AOP的实现分两大类SpringAOP和AspectJ; SpringAOP的代码实现可以使用注解方式，也可以使用配置XML的方式； 注解与XML方式，其实可以理解成利用不同的写法把这个特定的方法和这些通知组织起来，联系起来。 看过的好文章最后一部分提供两篇我看过的，很好的文章，希望对你也有所启迪。 AOP1这篇很简明的说明了SpringAOP与AspectJ的区别 AOP2这是一篇从AOP起源讲到不同实现的文章，几乎覆盖了我这篇，但我这是个完整例子。 我的代码 整个工程代码在下面的链接中，如果需要可以下载看一看。 点击跳转到我的GitHub]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>AspectJ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Spring中创建切面，通过切面引入新功能-使用配置XML]]></title>
    <url>%2F2018%2F11%2F14%2FaopXML%2F</url>
    <content type="text"><![CDATA[上一篇博客中记录了使用Java注解方式开发一个切面的小例子，这一篇记录使用XML配置的方式开发一个切面的例子，同时也完成通过配置XML新增功能。 实现切面定义特定的方法 首先定一个接口 12345package aopXML;public interface Performance &#123; void perform();&#125; 实现这个接口，定义一个叫演出的方法 12345678package aopXML;public class PerformanceImpl implements Performance &#123; @Override public void perform() &#123; System.out.println(&quot;演出～～～&quot;); &#125;&#125; 定义一个切面12345678910111213141516171819package aopXML;public class Audience &#123; public void silencePhone() &#123; System.out.println(&quot;手机静音！&quot;); &#125; public void takeSeats() &#123; System.out.println(&quot;请坐！&quot;); &#125; public void applause() &#123; System.out.println(&quot;表演结束，鼓掌！！！&quot;); &#125; public void demanRefund() &#123; System.out.println(&quot;表演失败，退票！！！&quot;); &#125;&#125; 将切面跟特定方法联系起来这一次采用配置XML的方式来开发；这个文件叫SpringAOP.xml定义切点，其实就是指定那个特定方法定义通知，其实就是通知对应的方法在特定方法前还是后调用，或者是特定方法调用成功或者出现异常123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd&quot;&gt; &lt;bean id=&quot;audience&quot; class=&quot;aopXML.Audience&quot; /&gt; &lt;bean id=&quot;Performance&quot; class=&quot;aopXML.PerformanceImpl&quot; /&gt; &lt;aop:config&gt; &lt;!-- 这是定义一个切面，切面是切点和通知的集合--&gt; &lt;aop:aspect id=&quot;do&quot; ref=&quot;audience&quot;&gt; &lt;!-- 定义切点 ，后面是expression语言，表示包括该接口中定义的所有方法都会被执行--&gt; &lt;aop:pointcut id=&quot;point&quot; expression=&quot;execution(* aopXML.Performance.perform(..)))&quot; /&gt; &lt;!-- 定义通知 --&gt; &lt;aop:before method=&quot;silencePhone&quot; pointcut-ref=&quot;point&quot; /&gt; &lt;aop:before method=&quot;takeSeats&quot; pointcut-ref=&quot;point&quot; /&gt; &lt;aop:after-returning method=&quot;applause&quot; pointcut-ref=&quot;point&quot; /&gt; &lt;aop:after-throwing method=&quot;demanRefund&quot; pointcut-ref=&quot;point&quot; /&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; 单元测试1234567891011121314151617181920package aopXML;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:SpringAOP.xml&quot;)public class PerformanceTest &#123; @Autowired private Performance performance; @Test public void test() &#123; performance.perform(); &#125;&#125; 输出结果：1234手机静音！请坐！演出～～～表演结束，鼓掌！！！ 新加功能定义新加功能 定义功能接口 12345package aopXML;public interface Encoreable &#123; void performEncore();&#125; 实现功能 12345678package aopXML;public class EncoreableImpl implements Encoreable &#123; @Override public void performEncore() &#123; System.out.println(&quot;返场表演～～～&quot;); &#125;&#125; 将新加的功能与特定方法联系起来修改SpringAOP.xml1234567891011121314151617181920212223242526&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.2.xsd&quot;&gt; &lt;bean id=&quot;audience&quot; class=&quot;aopXML.Audience&quot; /&gt; &lt;bean id=&quot;Performance&quot; class=&quot;aopXML.PerformanceImpl&quot; /&gt; &lt;bean id=&quot;Encoreable&quot; class=&quot;aopXML.EncoreableImpl&quot; /&gt; &lt;aop:config&gt; &lt;!-- 这是定义一个切面，切面是切点和通知的集合--&gt; &lt;aop:aspect id=&quot;do&quot; ref=&quot;audience&quot;&gt; &lt;!-- 定义切点 ，后面是expression语言，表示包括该接口中定义的所有方法都会被执行--&gt; &lt;aop:pointcut id=&quot;point&quot; expression=&quot;execution(* aopXML.Performance.perform(..)))&quot; /&gt; &lt;!-- 定义通知 --&gt; &lt;aop:before method=&quot;silencePhone&quot; pointcut-ref=&quot;point&quot; /&gt; &lt;aop:before method=&quot;takeSeats&quot; pointcut-ref=&quot;point&quot; /&gt; &lt;aop:after-returning method=&quot;applause&quot; pointcut-ref=&quot;point&quot; /&gt; &lt;aop:after-throwing method=&quot;demanRefund&quot; pointcut-ref=&quot;point&quot; /&gt; &lt;aop:declare-parents types-matching=&quot;aopXML.Performance+&quot; implement-interface=&quot;aopXML.Encoreable&quot; default-impl=&quot;aopXML.EncoreableImpl&quot;/&gt; &lt;/aop:aspect&gt; &lt;/aop:config&gt;&lt;/beans&gt; aop:declare-parents 使用这个标签将新加功能与特定方法联系在一起； types-matching=”aopXML.Performance+” 为所有实现Performance接口的类的父类增加新功能； implement-interface 新加功能对应着的接口； default-impl 新加功能接口的实现； 单元测试123456789101112131415161718192021222324package aopXML;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:SpringAOP.xml&quot;)public class PerformanceTest &#123; @Autowired private Performance performance; @Test public void test() &#123; performance.perform(); System.out.println(&quot;+++++++++++++++++++++++++++++++++++&quot;); Encoreable encoreable = (Encoreable)performance; encoreable.performEncore(); &#125;&#125; 输出结果：123456手机静音！请坐！演出～～～表演结束，鼓掌！！！+++++++++++++++++++++++++++++++++++返场表演～～～ 后记 整个工程代码在下面的链接中，如果需要可以下载看一看。 点击跳转到我的GitHub]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>XML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Spring中创建切面，通过切面引入新功能-使用JAVA注解方式]]></title>
    <url>%2F2018%2F11%2F13%2FaopJavaConfig%2F</url>
    <content type="text"><![CDATA[最近在看《Spring实战》，在这儿使用注解完整的实现一个切面的例子，也实现通过注解引入新功能； 实现切面关于切面相关概念这篇不提，可以大致理解成，只要调用某个特定的方法，这个调用信息会被切面拦截，然后执行切面定义的逻辑，之后才能顺利的调用该方法。我这个是一个maven项目所有代码写在同一个包下面，测试类除外。关于切面这个部分可能需要导入一些包。pom.xml12345678910&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.8.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.2.4&lt;/version&gt;&lt;/dependency&gt; 定义特定的方法 首先定一个接口 12345package aopJavaConfig;public interface Performance &#123; public void perform();&#125; 实现这个接口，定义一个叫演出的方法 12345678910package aopJavaConfig;import org.springframework.stereotype.Component;@Componentpublic class PerformanceImpl implements Performance &#123; @Override public void perform()&#123; System.out.println(&quot;演出～～～&quot;); &#125;&#125; 定义一个切面，同时定义了切点和通知1234567891011121314151617181920212223242526272829package aopJavaConfig;import org.aspectj.lang.annotation.*;@Aspectpublic class Audience &#123; @Pointcut(&quot;execution(* aopJavaConfig.Performance.perform(..))&quot;) public void performance()&#123;&#125; @Before(&quot;performance()&quot;) public void silencePhone()&#123; System.out.println(&quot;手机静音！&quot;); &#125; @Before(&quot;performance()&quot;) public void takeSeats()&#123; System.out.println(&quot;请坐！&quot;); &#125; @AfterReturning(&quot;performance()&quot;) public void applause()&#123; System.out.println(&quot;表演结束，鼓掌！！！&quot;); &#125; @AfterThrowing(&quot;performance()&quot;) public void demanRefund()&#123; System.out.println(&quot;表演失败，退票！！！&quot;); &#125;&#125; @Aspect修饰的这个类是切面，我们可以在切面中定义切点和通知； @Pointcut修饰的是切点，可以看到切点就是我们之前定义的那个特定的方法； @Before修饰的方法会在定义的特定方法之前被调用； @AfterReturning修饰的方法会在特定方法被成功调用后执行； @AfterThrowing修饰的方法会在特定方法执行异常时被执行； 定义一个配置类这个配置类负责开启自动代理，并且将我们定义好的切面声明成Bean；1234567891011121314151617package aopJavaConfig;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.EnableAspectJAutoProxy;@Configuration@EnableAspectJAutoProxy@ComponentScanpublic class ConcertConfig &#123; @Bean public Audience audience()&#123; return new Audience(); &#125;&#125; 至此，我们有两个bean，一个是那个特定的方法，一个是声明好的切面，那么我们就有一个很朴素的冲动，这样定义是否成功了呢，所以写一个测试类； 测试切面定义是否成功预期的结果：调用演出方法，在控制台打印出来演出之前要干嘛，演出成功后要干嘛；具体的输出请查看代码本身；12345678910111213141516171819package aopJavaConfig;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = ConcertConfig.class)public class PerformanceTest &#123; @Autowired private Performance performance; @Test public void test() &#123; performance.perform(); &#125;&#125; 至此，使用java注解方式定义一个切面结束； 引入新功能（DeclareParents）接上文代码，当我们接受老代码，期望在原来的特定方法的基础上加一个功能，很朴素的想法就是修改原来的方法，加一段功能代码，但是假设现在老代码很复杂不得修改；那么为了增加这个功能，Spring提供了一个另一个实现方式。 新功能代码 定义新接口 12345package aopJavaConfig;public interface Encoreable &#123; void performEncore();&#125; 实现这个接口 12345678910package aopJavaConfig;import org.springframework.stereotype.Component;@Componentpublic class EncoreableImpl implements Encoreable&#123; @Override public void performEncore()&#123; System.out.println(&quot;返场表演～～～&quot;); &#125;&#125; 将新功能引入原来特定方法形成的bean中可以将bean看作一个对象，这个特定的这个对象中添加一个新的方法，也就是让这个对象可以直接调用新加的方法；123456789101112package aopJavaConfig;import org.aspectj.lang.annotation.Aspect;import org.aspectj.lang.annotation.DeclareParents;@Aspectpublic class EncoreableIntroducer &#123; /** * 此注解可以将新加的接口引入到原来调用目标函数的bean中 */ @DeclareParents(value = &quot;aopJavaConfig.Performance+&quot;, defaultImpl = EncoreableImpl.class) public static Encoreable encoreable;&#125; @DeclareParents 这个注解的value制定了这个新方法加给谁，defaultImpl这个参数指定了新加方法的具体实现。实际上新加方法是加给了value制定方法的父类。新加方法要告知Spring容器修改配置类如下：12345678910111213141516171819202122package aopJavaConfig;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.EnableAspectJAutoProxy;@Configuration@EnableAspectJAutoProxy@ComponentScanpublic class ConcertConfig &#123; @Bean public Audience audience()&#123; return new Audience(); &#125; @Bean public EncoreableIntroducer encoreableIntroducer()&#123; return new EncoreableIntroducer(); &#125;&#125; 测试类预期结果：能够使用特定类的对象成功调用新加的方法；12345678910111213141516171819202122package aopJavaConfig;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = ConcertConfig.class)public class PerformanceTest &#123; @Autowired private Performance performance; @Test public void test() &#123; performance.perform(); System.out.println(&quot;+++++++++++++++++++++++++++++++++++&quot;); Encoreable encoreable = (Encoreable)performance; encoreable.performEncore(); &#125;&#125; 测试结果如下:123456手机静音！请坐！演出～～～表演结束，鼓掌！！！+++++++++++++++++++++++++++++++++++返场表演～～～ 至此使用注解方式开发切面，切点，通知，新加方法都已经完成。代码经过了验证；如有错误，还请指正。欢迎交流。 后记 整个工程代码在下面的链接中，如果需要可以下载看一看。 点击跳转到我的GitHub]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
        <tag>注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell中嵌入SQL查询]]></title>
    <url>%2F2018%2F08%2F29%2FshellPit01%2F</url>
    <content type="text"><![CDATA[这两天在做一个hadoop升级的事情。分配到我的任务就是在新的集群测试项目中的脚本，保证能够在新集群跑得通，并产生正确的数据。那么数据正不正确怎么个比较法呢，因为暂时是新旧集群一起运行此项目，项目最后产出的数据量也不大，十万过一点，四五个文件。干脆down下来用beyond Compare这个软件比较一下算了。但在down下来的过程中踩坑了。 问题复述因为种种限制，我不能直接将两个数据库中的数据导出来做比较，但是任何查询都是可以的，那么最简单的想法就是写个脚本，根据不同的传参，配置不同的数据库信息，导出数据。脚本中最重要的就是怎么把SQL嵌入脚本。说来简单，我是这样写的：123sql=&quot;select * from test.info;&quot;mysql=&quot;mysql -h$HOST -p$PORT -u$USER -p$PASSWD&quot;echo $&#123;sql&#125; | $&#123;mysql&#125; 第一行查询语句；第二行数据库登录信息，ip地址，用户，密码；第三行使用登录信息进行这个sql查询。执行报错，根据报错看出来是语法问题，select 附近有错。在第三行echo ${sql}，发现这个select后面的*，居然被替换成了当前脚本所在目录下的所有文件名字。怎么个意思呢？假设/user/local下有a.txt b.txt c.sh三个文件，你在c.sh中写了如上（就那个select * ）的代码，那么你的查询语句中的*就会被替换成a.txt b.txt; 解决方法将echo ${sql} | ${mysql}改为echo &quot;${sql}&quot; | ${mysql}没看错就是在变量${sql}外面包了一层双引号，就这么解决了。 脑洞大开由于没有查到翔实的资料，以下都是个人猜测。将脚本写成下面这个样子：12345 #!/bin/bashsql=&quot;select * from test.klhinfo;&quot;mysql=&quot;mysql -uroot -pmysql123&quot;echo $&#123;sql&#125;echo &quot;$&#123;sql&#125;&quot; | $&#123;mysql&#125; 然后运行一下，发现当echo ${sql}时，其中的星号被替换成了当前目录下的文件名。脑洞大开：${sql} 被替换成 $select $* $from $test.klhinfo, $*正好是获取所有传入的参数，估计是将当前目录下的文件名都当作参数传进来了。 在shell脚本中嵌入SQL的姿势我所了解的暂时有三种方式，在生产环境中第二种方式使用最多，第一种也常见，第三种比较少见。 shell中echo方式 12345 #!/bin/bashsql=&quot;select * from test.klhinfo;&quot;mysql=&quot;mysql -uroot -pmysql123&quot;echo &quot;$&#123;sql&#125;&quot;echo &quot;$&#123;sql&#125;&quot; | $&#123;mysql&#125; mysql命令行方式 12345#!/bin/bashsql=&quot;select * from test.klhinfo;&quot;mysql=&quot;mysql -uroot -pmysql123&quot;echo &quot;$&#123;sql&#125;&quot;$&#123;mysql&#125; -N -e &quot;$&#123;sql&#125;&quot; 使用EOF直接嵌入 123456#!/bin/bashpasswd=&apos;mysql123&apos;mysql -u root -p$&#123;passwd&#125; &lt;&lt;EOFuse kaka;select * from user;EOF]]></content>
      <categories>
        <category>shell</category>
      </categories>
      <tags>
        <tag>shell sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mapreduce进程ruduce端内存溢出，解决方法和探索]]></title>
    <url>%2F2018%2F08%2F16%2FPigOOM1%2F</url>
    <content type="text"><![CDATA[昨天碰到一个pig任务执行过程中发生了内存溢出。写点文字记录一下解决过程，顺便整理一下自己的思路。 一 错误信息1234567891011121314151617181920212223242018-08-15 05:20:24,102 [main] INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 75% complete2018-08-15 05:20:58,720 [main] WARN org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Ooops! Some job has failed! Specify -stop_on_failure if you want Pig to stop immediately on failure.2018-08-15 05:20:58,721 [main] INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - job job_1534254968444_0056 has failed! Stop running all dependent jobs2018-08-15 05:20:58,721 [main] INFO org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete2018-08-15 05:20:59,123 [main] ERROR org.apache.pig.tools.pigstats.SimplePigStats - ERROR 2997: Unable to recreate exception from backed error: AttemptID:attempt_1534254968444_0056_r_000000_3 Info:Error: org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#12 at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Caused by: java.lang.OutOfMemoryError: Java heap space at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:56) at org.apache.hadoop.io.BoundedByteArrayOutputStream.&lt;init&gt;(BoundedByteArrayOutputStream.java:46) at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.&lt;init&gt;(InMemoryMapOutput.java:63) at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.unconditionalReserve(MergeManagerImpl.java:309) at org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:299) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:539) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:348) at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:198)2018-08-15 05:20:59,123 [main] ERROR org.apache.pig.tools.pigstats.PigStatsUtil - 1 map reduce job(s) failed!2018-08-15 05:20:59,125 [main] INFO org.apache.pig.tools.pigstats.SimplePigStats - Script Statistics: 二 解决方法看到mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#12和Caused by: java.lang.OutOfMemoryError: Java heap space；就可以断定是在Reduce端的shuffer过程中内存不足，简单粗暴的做法就是加大MR过程中reduce端的内存。具体涉及到的参数如下： mapreduce.reduce.memory.mb // 这个是为reduce端分配的内存大小，一般来说是1024的倍数。如果你分配了2000m，系统也会为你分配2048m. mapreduce.reduce.java.opts // 这个是指定reduce端的JVM参数，这个的大小一般是上一个参数的0.75倍，因为要剩一些内存给非JVM进程。具体做法因为在项目中是在一个shell脚本中用命令行调用了一个pig脚本。$PIG_HOME/bin/pig -param YESTERDAY=$YESTERDAY -param FUNCTION=$FUNCTION/$TASK_PROGRAM_DIR/example.pig那么就会产生一个问题，我们解决方法中提到的两个配置参数，插入那个脚本合适？经过测试以上两个参数配置在shell脚本中是不起作用的。只能配置在pig脚本中。pig中使用set语法如下：12SET mapreduce.reduce.memory.mb 3072;SET mapreduce.reduce.java.opts &apos;-Xms3000m -Xmx3000m&apos;; 具体配置参数的大小要结合上一次运行时内存分配的大小和数据量的大小，然后自己预估一个合适的值。不行就继续调整。直到问题解决。 三 探讨我们以上给出的方法是一种土财主的做法，基于我们有充足的内存。再看报错信息error in shuffle in fetcher#12，发现是在下载map输出时内存不够了。Reduce在shuffle阶段对下载来的map数据，并不是立刻就写入磁盘的，而是会先缓存在内存中，然后当使用内存达到一定量的时候才刷入磁盘。那么就有可能是某个map输出过大，下载过来直接撑爆了内存。引入两个参数： 参数: mapred.job.shuffle.input.buffer.percent（default 0.7）说明: 用来缓存shuffle数据的reduce task heap百分比这个参数其实是一个百分比，意思是说，shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task。缓存的数据量超过了这个值，便开始将缓存数据写入磁盘。 参数：mapred.job.shuffle.merge.percent（default 0.66）说明:缓存的内存中多少百分比后开始做merge操作假设mapred.job.shuffle.input.buffer.percent为0.7，reduce task的max heapsize为1G，那么用来做下载数据缓存的内存就为大概700MB左右，这700M的内存，也不是要等到全部写满才会往磁盘刷的，而是当这700M中被使用到了一定的限度（通常是一个百分比），就会开始往磁盘刷。假设mapred.job.shuffle.merge.percent（default 0.66）为0.66，那么当下载过来的map的输出在缓存中达到 0.7 maxHeap 0.66这个值，缓存的数据就开始往磁盘中写。个人理解这就是两个关卡，前者决定用内存的百分之多少来做数据缓存，后者决定缓存使用到多少就开始往磁盘写数据。 探讨结果当我们机器本身的内存有限，或者我们追求内存的利用率。就可以放弃从整体上扩大内存的做法，转而调整任务各个阶段使用内存的比例。在这次报错中我们就可以尝试将下面两个参数调小一点，mapred.job.shuffle.input.buffer.percent // 这个调小，是将reduce端用来做数据缓存的内存减少。防止下载一个过大的map输出直接撑爆内存，导致任务失败。mapred.job.shuffle.merge.percent // 这个调小是在上一个的基础上，进一步提前了将缓存的数据写入磁盘的时间。 如有错误，欢迎指出，十分感谢。参考文章：MapReduce优化—-参数的解释以及设置Yarn下Mapreduce的内存参数理解]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>pig</tag>
        <tag>mapreduce</tag>
        <tag>OOM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase ImportTSV]]></title>
    <url>%2F2018%2F08%2F14%2FHbaseImportTSV%2F</url>
    <content type="text"><![CDATA[这个也是最近经手的一个项目中涉及到的一个可以将HDFS上的数据直接导入HBASE表中的命令行工具。这个属于HBASE所以与上一篇Hadoop中的工具分开来写。 Hbase importTsv概述和使用步骤：ImportTsv是Hbase提供的一个命令行工具，可以将存储在HDFS上的自定义分隔符（默认\t）的数据文件，通过一条命令方便的导入到HBase表中，对于大数据量导入非常实用，其中包含两种方式将数据导入到HBase表中：stepA: 生成HFile格式的文件stepB: 执行一个叫做CompleteBulkLoad的命令，将文件move到HBase表空间目录 一个例子： 第一步，生成Hfile 12345$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,f:accountID,f:time,f:crownCommonKeyID //格式-Dimporttsv.bulk.output=hdfs://storefile-outputdir //输出目录&lt;tablename&gt; //输出后保存文件名&lt;hdfs-data-inputdir&gt; //输入目录 第二步，完成导入 123$ bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles &lt;hdfs://storefileoutput&gt; //这是上文的输出目录，也是这一步的输入目录&lt;tablename&gt; //导入hbase的表名 基本使用便是以上这样子。]]></content>
      <categories>
        <category>Hbase</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>import</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop Streamig 和 Hadoop Distcp的简单介绍]]></title>
    <url>%2F2018%2F08%2F14%2FStreamingAndDistcp%2F</url>
    <content type="text"><![CDATA[今天总结一下经手的一个项目中用到的Hadoop工具；距离上一篇文章不知不觉快4个月过去了，期间经历了毕业，入职这些事儿。一直想写个求职总结，错过了当初那份激情，现在已经有点淡忘那种真真切切的朝不保夕的感受。看后来有没有心情在更吧。我毕设也挺好玩，挺简单的一个东西，有时间也可以写写。这都是后话。 Hadoop Streaming概述与基本使用：Hadoop streaming是Hadoop的一个工具， 它帮助用户创建和运行一类特殊的map/reduce作业， 这些特殊的map/reduce作业是由一些可执行文件或脚本文件充当mapper或者reducer。例如：123456$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/hadoop-streaming.jar \ -input myInputDirs \ -output myOutputDir \ -mapper /bin/cat \ -reducer /bin/wc -file /home/Waterkong/test.sh 个人理解 -mapper与-reducer都可以指定一个可执行文件（可以是脚本，也可以是class文件），如果不需要map/reduce任务则可以省略对应的这个选项。 -file 任何可执行文件都可以被指定为mapper/reducer。这些可执行文件不需要事先存放在集群上； 如果在集群上还没有，则需要用-file选项让framework把可执行文件作为作业的一部分，一起打包提交。 一个说明图，说明这些作为Map或者Reduce的脚本与hadoop起的进程之间是怎么协作的。最重要的就是这些可执行文件是另起进程的。 以上就是一个简单的介绍，Hadoop Streaming具体介绍可以点击下面的链接点击查看Hadoop Streaming的详细介绍 Hadoop Distcp概述：DistCp（分布式拷贝）是用于大规模集群内部和集群之间拷贝的工具。 它使用Map/Reduce实现文件分发，错误处理和恢复，以及报告生成。 它把文件和目录的列表作为map任务的输入，每个任务会完成源列表中部分文件的拷贝。 由于使用了Map/Reduce方法，这个工具在语义和执行上都会有特殊的地方。 这篇文档会为常用DistCp操作提供指南并阐述它的工作模型。 基本使用方法：DistCp最常用在集群之间的拷贝：12bash$ hadoop distcp hdfs://nn1:8020/foo/bar \ hdfs://nn2:8020/bar/foo 这条命令会把nn1集群的/foo/bar目录下的所有文件或目录名展开并存储到一个临时文件中，这些文件内容的拷贝工作被分配给多个map任务， 然后每个TaskTracker分别执行从nn1到nn2的拷贝操作。注意DistCp使用绝对路径进行操作。命令行中可以指定多个源目录：123bash$ hadoop distcp hdfs://nn1:8020/foo/a \ hdfs://nn1:8020/foo/b \ hdfs://nn2:8020/bar/foo 或者使用-f选项，从文件里获得多个源：12bash$ hadoop distcp -f hdfs://nn1:8020/srclist \ hdfs://nn2:8020/bar/foo 其中srclist 的内容是12hdfs://nn1:8020/foo/a hdfs://nn1:8020/foo/b 一个问题问题：源目录之前的hdfs://是可以省略的，但是省略后他默认的HDFS的地址是怎么找到的？因为是Hadoop的工具所以去默认的Hadoop环境找,在core-site.xml发现了默认的文件系统(HDFS)的地址:1234&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://Waterkong&lt;/value&gt;&lt;/property&gt; 但后面的这个hdfs://Waterkong是ping不通的。因为配置了高可用，真正投入使用的namenode的地址还指不定是哪一个，所以这个真正的地址还要去hdfs-site.xml中查找现在正在使用的namanode的地址。12345678&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.Waterkong.nn1&lt;/name&gt;&lt;value&gt;rsync.nn1.Waterkong.zw.ted:50070&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.Waterkong.nn2&lt;/name&gt;&lt;value&gt;rsync.nn2.Waterkong.sjs.ted:50070&lt;/value&gt;&lt;/property&gt; 以上是HA节点真正的地址，至此客户端与服务端交互地址被找到。（这儿的nn1与nn2分别代表主备namenode，与上面的例子中的nn1,nn2没有关系。）以上就是一个简单的介绍，Hadoop Distcp具体介绍可以点击下面的链接点击查看Hadoop Distcp的详细介绍如有错误，敬请指导。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>Streaming</tag>
        <tag>Distcp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一点资讯技术面总结]]></title>
    <url>%2F2018%2F04%2F19%2FYiDianZiXun%2F</url>
    <content type="text"><![CDATA[感谢一点资讯给的面试机会；以前文章提到过的不在赘述，记录新的知识点。 LinuxLinux命令用法查询网站：Click to jump wcwc统计文件里面有多少单词，多少行，多少字符。语法:[root@www ~]# wc [-lwm]选项与参数：-l ：仅列出行；-w ：仅列出多少字(英文单字)；-m ：多少字符；eg:默认使用wc统计/etc/passwd #wc /etc/passwd40 45 1719 /etc/passwd40是行数，45是单词数，1719是字节数 #wc -l /etc/passwd #统计行数，在对记录数时，很常用40 /etc/passwd #表示系统有40个账户 #wc -w /etc/passwd #统计单词出现次数45 /etc/passwd #wc -m /etc/passwd #统计文件的字符数1719grepgrep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。常见用法： 在文件中搜索一个单词，命令会返回一个包含“match_pattern”的文本行：grep match_pattern file_namegrep “match_pattern” file_name在多个文件中查找：grep “match_pattern” file_1 file_2 file_3 … 输出除了匹配到的之外的所有行 -v 选项：grep -v “match_pattern” file_name 使用正则表达式 -E 选项：grep -E “[1-9]+”或egrep “[1-9]+” 统计文件或者文本中包含匹配字符串的行数 -c 选项：grep -c “text” file_name 输出包含匹配字符串的行数 -n 选项：grep “text” -n file_name或cat file_name | grep “text” -n#多个文件grep “text” -n file_1 file_2date 格式化输出：date + “%Y%m%d” 前一天：date -d “1 day ago” + date +”%Y-%m-%d” 加减操作：date +%Y%m%d //显示前天年月日date -d “+1 day” +%Y%m%d //显示前一天的日期date -d “-1 day” +%Y%m%d //显示后一天的日期date -d “-1 month” +%Y%m%d //显示上一月的日期date -d “+1 month” +%Y%m%d //显示下一月的日期date -d “-1 year” +%Y%m%d //显示前一年的日期date -d “+1 year” +%Y%m%d //显示下一年的日期 设定时间：date -s //设置当前时间，只有root权限才能设置，其他只能查看date -s 20120523 //设置成20120523，这样会把具体时间设置成空00:00:00date -s 01:01:01 //设置具体时间，不会对日期做更改date -s “01:01:01 2012-05-23” //这样可以设置全部时间date -s “01:01:01 20120523” //这样可以设置全部时间date -s “2012-05-23 01:01:01” //这样可以设置全部时间date -s “20120523 01:01:01” //这样可以设置全部时间crontab用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab文件都被保存在/var/spool/cron目录中。其文件名与用户名一致，使用者权限文件如下：/etc/cron.deny 该文件中所列用户不允许使用crontab命令/etc/cron.allow 该文件中所列用户允许使用crontab命令/var/spool/cron/ 所有用户crontab文件存放的目录,以用户名命名 crontab文件的含义：用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： minute hour day month week command 顺序：分 时 日 月 周 其中：minute： 表示分钟，可以是从0到59之间的任何整数。hour：表示小时，可以是从0到23之间的任何整数。day：表示日期，可以是从1到31之间的任何整数。month：表示月份，可以是从1到12之间的任何整数。week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。在以上各个字段中，还可以使用以下特殊字符：星号（）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9”中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6”正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如/10，如果用在minute字段，表示每十分钟执行一次。详细用法链接： 数据结构与算法给出一个数组{0，1，1，1，2，2，3，3，4，4，5，6，7，8，8}，求出现次数最多的那个数字出现次数，显然应该返回1，出现3次；当时的想法是采用桶，给出数组中的元素做下标，出现一次，对应桶的值+1；但有问题，如果给出的数组特别大，那么空间复杂度就高，甚至内存溢出；所以采取新的思路，用HashMap&lt;Integer, Integer&gt;,数组元素做key，出现次数做value;遍历数组，出现一次，value++；代码：1234567891011121314151617181920212223242526272829303132package YiDianZiXun;import java.util.HashMap;// 找到出现次数最多的那个数，和出现的次数；public class CountTimes &#123; public static void main(String[] args) &#123; int[] arr = &#123; 0, 1, 1, 1, 1, 2, 2, 3, 3, 4, 4, 5, 6, 7, 8, 8 &#125;; int[] res = MaxTimes(arr); System.out.println(&quot;数字：&quot; + res[0] + &quot; 出现次数： &quot; + res[1]); &#125; private static int[] MaxTimes(int[] arr) &#123; if(arr == null || arr.length &lt; 1) return new int[] &#123;&#125;; HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); int times = 0; int num = 0; for (int i = 0; i &lt; arr.length; i++) &#123; if (map.get(arr[i]) == null) &#123; map.put(arr[i], 1); &#125; else &#123; Integer value = map.get(arr[i]) + 1; map.put(arr[i], value); if (value &gt; times) &#123; times = value; num = arr[i]; &#125; &#125; &#125; return new int[] &#123; num, times &#125;; &#125;&#125; 给出一个数组{-1，0，1，2，3，0，1，4，-1，0}，求和为零的最长连续子序列；思路：定义sum(i)是数组的前i项和；我们假设从i+1位置到j位置的和为零，那么我们可以得到sum(i) == sum(j)；计算sum(i)，用一个map记录这个和，以及这个和第一次出现的位置；如果一个和不是第一次出现，也就是存在sum(i) == sum(j)；j &gt; i；那么从i+1 到 j位置的和就是零；长度为(j - i);代码：1234567891011121314151617181920212223242526272829303132package YiDianZiXun;import java.util.HashMap;// 求一个数组所有子数组中和为零子数组长度最长的。public class SubArrIsZero &#123; public static void main(String[] args) &#123; int[] arr = &#123; -1, 0, 1, 2, 3, 0, 1, 4, -1, 0 &#125;; int len = GetMaxLength(arr); System.out.println(len); &#125; private static int GetMaxLength(int[] arr) &#123; if (arr == null || arr.length &lt; 1) return 0; HashMap&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); // 为了解决从开头第一个位置有一个子数组就是所求 map.put(0, -1); int sum = 0; int len = 0; for (int i = 0; i &lt; arr.length; i++) &#123; sum += arr[i]; if (!map.containsKey(sum)) &#123; map.put(sum, i); &#125; else &#123; int left = map.get(sum); len = Math.max(len, (i - left)); &#125; &#125; return len; &#125;&#125; 手撸快排排序的重要性不言而喻，在我有限的面试中就已经手写了两次快排了;归并，堆排序，都需要掌握； 一个总结递推公式的题：这类题只要有递推公式以及边界条件，剩下的就是代码优化；有一个 2xN 的区域，现在用N个2x1的矩形去填充，问总共有多少种方式？破题点：最后一块怎么放？分情况：最后一块竖着放，那么使用前n-1块的形成的方案数量等于用n块的方案数量；最后一块横着放，那么第n-1块必然也是横着放，所以使用前n-2块形成的方案数量等于使用n块的方案数量；那么总方案数量等于以上两种情况的和：f(n) = f(n-1) + f(n-2);且f(1) = 1, f(2) = 2;代码：1234567891011121314151617181920212223242526package YiDianZiXun;// 矩形覆盖问题public class Fibonacci &#123; public static void main(String[] args) &#123; int n = 3; int res = RectCover(n); System.out.println(res); &#125; private static int RectCover(int target) &#123; if (target == 0) return 1; if (target &lt; 3) return target; int pre = 1; int cur = 2; int res = 0; for (int i = 3; i &lt;= target; i++) &#123; res = pre + cur; pre = cur; cur = res; &#125; return res; &#125;&#125; SQLbetween and 用法；]]></content>
      <categories>
        <category>Interview</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜狗技术面总结]]></title>
    <url>%2F2018%2F04%2F18%2FSouGou%2F</url>
    <content type="text"><![CDATA[感谢搜狗给的面试机会，感谢技术面大佬；基本按着简历问的，这篇整理以前面试中提到的不在赘述；记录一些新的或者没注意到得细节； HiveHQL中的join与MR的转换map阶段整体上来说就是将表中的每一条记录，转换为key-value形式；一般都是on后的条件字段做key，其他字段做value，但是value中要加一个字段就是当前记录来自那张表；shuffle阶段：将相同key的记录整理到一起；reduce阶段： 将拥有相同key，但来自不同表，的字段进行整合，做成一条记录； 分区的认识Hive中的join操作会扫描全表，当表比较大时，这样扫描耗时长，效率低；引进分区，可以理解为是将表进行了划分，我们可以在join操作是指定分区，这样就只扫描指定的分区，不做全表扫描，节约时间，提高效率； hadoop客户端向HDFS写操作中间如果发生了失败，后续是怎么处理的；整个写流程如下： 第一步，客户端调用DistributedFileSystem的create()方法，开始创建新文件：DistributedFileSystem创建DFSOutputStream，产生一个RPC调用，让NameNode在文件系统的命名空间中创建这一新文件； 第二步，NameNode接收到用户的写文件的RPC请求后，谁偶先要执行各种检查，如客户是否有相关的创佳权限和该文件是否已存在等，检查都通过后才会创建一个新文件，并将操作记录到编辑日志，然后DistributedFileSystem会将DFSOutputStream对象包装在FSDataOutStream实例中，返回客户端；否则文件创建失败并且给客户端抛IOException。 第三步，客户端开始写文件：DFSOutputStream会将文件分割成packets数据包，然后将这些packets写到其内部的一个叫做data queue(数据队列)。data queue会向NameNode节点请求适合存储数据副本的DataNode节点的列表，然后这些DataNode之前生成一个Pipeline数据流管道，我们假设副本集参数被设置为3，那么这个数据流管道中就有三个DataNode节点。 第四步，首先DFSOutputStream会将packets向Pipeline数据流管道中的第一个DataNode节点写数据，第一个DataNode接收packets然后把packets写向Pipeline中的第二个节点，同理，第二个节点保存接收到的数据然后将数据写向Pipeline中的第三个DataNode节点。 第五步，DFSOutputStream内部同样维护另外一个内部的写数据确认队列——ack queue。当Pipeline中的第三个DataNode节点将packets成功保存后，该节点回向第二个DataNode返回一个确认数据写成功的信息，第二个DataNode接收到该确认信息后在当前节点数据写成功后也会向Pipeline中第一个DataNode节点发送一个确认数据写成功的信息，然后第一个节点在收到该信息后如果该节点的数据也写成功后，会将packets从ack queue中将数据删除。在写数据的过程中，如果Pipeline数据流管道中的一个DataNode节点写失败了会发生什问题、需要做哪些内部处理呢？如果这种情况发生，那么就会执行一些操作：首先，Pipeline数据流管道会被关闭，ack queue中的packets会被添加到data queue的前面以确保不会发生packets数据包的丢失；接着，在正常的DataNode节点上的以保存好的block的ID版本会升级——这样发生故障的DataNode节点上的block数据会在节点恢复正常后被删除，失效节点也会被从Pipeline中删除；最后，剩下的数据会被写入到Pipeline数据流管道中的其他两个节点中。如果Pipeline中的多个节点在写数据是发生失败，那么只要写成功的block的数量达到dfs.replication.min(默认为1)，那么就任务是写成功的，然后NameNode后通过一步的方式将block复制到其他节点，最后事数据副本达到dfs.replication参数配置的个数。 第六步，，完成写操作后，客户端调用close()关闭写操作，刷新数据； 第七步，，在数据刷新完后NameNode后关闭写操作流。到此，整个写操作完成。 HDFS的HA（高可用）实现中采用了QJM，详细介绍一下；一句话：通过共享存储，共享了编辑日志；怎么保证了高可用，活动的NN写编辑日志需要写入大多数JN节点，才能完成写操作；这样就可以保证编辑日志是同步的，所以备用NN只要拿到编辑日志，然后根据编辑日志，重做元数据，就能到的最新的元数据，保证了高可用； 比较详细的解释：在高可用配置下，editlog不再存放在名称节点，而是存放在一个共享存储的地方，这个共享存储由奇数个Journal Node组成，一般是3个节点(JN小集群)， 每个JN专门用于存放来自NN的编辑日志，编辑日志由活跃状态的名称节点写入JN小集群。那么要有2个NN，而且二者之中只能有一个NN处于活跃状态（active），另一个是待命状态（standby），只有active的NN节点才能对外提供读写HDFS服务，也只有active态的NN才能向JN写入编辑日志；standby状态的NN负责从JN小集群中拷贝数据（这个数据就是编辑日志）到本地。另外，各个DATA NODE也要同时向两个名称节点报告状态(心跳信息、块信息) JAVAHashMap为什么线程不安全，或者说hashMap的线程不安全表现在哪； 多线程下Resize操作，可能会形成链表环Hashmap在插入元素过多的时候需要进行Resize，Resize的条件是HashMap.Size &gt;= Capacity * LoadFactor。Hashmap的Resize包含扩容和ReHash两个步骤，ReHash在并发的情况下可能会形成链表环点击查看形成链表环的详细解释 同时addEntry两个线程同时获得了数组的一个位置，对一个Entry进行put操作，两个线程都有可能先执行，那么一个线程必然覆盖另一个线程的写入结果。安全的方式是不能同时获得同一个节点，写操作不能相互覆盖； 将M个长度为N的有序数组进行Merge，这个过程的时间复杂度；要将M个有序数组合并，我的想法是一直两两合并，直到最后合并成一个，一对（两个）有序数组合并，他们要执行比较语句2N次，第一步：有M/2对，要比较2N次，M/2 * 2N = MN;第二步：有M/4对，要比较4N次（合并后长度是上一次的两倍），M/4 * 4N = MN;…直到合并成一个，这是一个分步计算，所以最终比较次数是以上每一步相加的和；又因为大O表示法只取最高次项，所以最终这个时间复杂度为：O(MN) java 的IO流如果不关闭会造成什么后果；占用资源，内存溢出；]]></content>
      <categories>
        <category>Interview</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[创智优品面试总结]]></title>
    <url>%2F2018%2F04%2F10%2FZonjey%2F</url>
    <content type="text"><![CDATA[4.08号，创智优品面试，学长内推，一面被拒。两个算法题； 数据结构与算法题目一：给你一棵二叉树，找到距离最远的两个节点；（从A节点出发，可以向上或者向下走，沿途的节点只能经过一次，当到达B节点时，路径上的节点数叫做A到B的距离）思路：分三种情况假设给出的根为root root的左子树上的最大距离； root的右子树上的最大距离； 跨越root，左子树距离root的最远距离 + 1 + 右子树距离root最远的距离；在这三个距离中取最大的一个就是所求；代码：12345678910111213141516171819// 我们需要一个独立于递归函数之外的变量，不受递归影响，还要递归函数能够改变，所以采用了一个record；private static int maxDistance(Node head) &#123; int[] record = new int[1]; return posOrder(head, record);&#125;// 递归函数，设置递归，确定边界；private static int posOrder(Node head, int[] record) &#123; if(head == null) &#123; record[0] = 0; return 0; &#125; int lmax = posOrder(head.left, record); int maxfromLeft = record[0]; int rmax = posOrder(head.right, record); int maxfromRight = record[0]; int curNodeMax = maxfromLeft + maxfromRight + 1; record[0] = Math.max(maxfromLeft, maxfromRight) + 1; return Math.max(Math.max(lmax, rmax), curNodeMax);&#125; 题目二：手写快速排序；这个就直接上代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344import java.util.Arrays;public class quick_sort &#123; public static void main(String[] args) &#123; int[] arr = &#123;1,4,7,2,5,8,3,6,9,5,4&#125;; quickSort(arr, 0, arr.length-1); System.out.println(&quot;最终结果：&quot; + Arrays.toString(arr)); &#125; private static void quickSort(int[] arr, int i, int j) &#123; if(i &gt;= j) return ; else &#123; int index = partition(arr, i, j); System.out.println(&quot;index: &quot; + index); quickSort(arr, i, index-1); quickSort(arr, index+1, j); &#125; &#125; // 返回基准的下标 private static int partition(int[] arr, int low, int heigh) &#123; int key = arr[low]; while(low &lt; heigh) &#123; // 这儿要说明的一点是：必须先从后往前比，因为前面用key记住了基准，空开了一个位置 // 所以从后向前比较发现一个小于基准的数字，放在这个空开的位置； while(arr[heigh] &gt;= key &amp;&amp; low &lt; heigh) &#123; heigh--; &#125; // 在后面找到了小于基准的数，放在基准前；同时后面空出了一个位置，所以从前面找一个 // 大于基准的数，放在这个位置上； arr[low] = arr[heigh]; //从前向后比较 while(arr[low] &lt;= key &amp;&amp; low &lt; heigh) &#123; low++; &#125; // 这儿就是在前面发现了一个大于基准的数，把他放在后面的位置上； arr[heigh] = arr[low]; // 至此，小于基准的位于一边，大于基准的位于另一边； System.out.println(&quot;前后置换一次：&quot; + Arrays.toString(arr)); &#125; arr[heigh] = key; return heigh; &#125;&#125; Javajava垃圾回收算法 标记 - 清除将需要回收的对象进行标记，然后清除。不足：两点：效率问题和时间空间问题 标记和清除过程效率都不高 会产生大量碎片，内存碎片过多可能导致无法给大对象分配内存之后的算法都是基于该算法进行改进。 复制 将内存划分为大小相等的两块，每次只使用其中一块，当这一块内存用完了就将还存活的对象复制到另一块上面，然后再把使用过的内存空间进行一次清理。 主要不足是只使用了内存的一半。 现在的商业虚拟机都采用这种收集算法来回收新生代，但是并不是将内存划分为大小相等的两块，而是分为一块较大的 Eden 空间和两块较小的 Survior 空间，每次使用 Eden 空间和其中一块 Survivor。在回收时，将 Eden 和 Survivor 中还存活着的对象一次性复制到另一块 Survivor 空间上，最后清理 Eden 和 使用过的那一块 Survivor。HotSpot 虚拟机的 Eden 和 Survivor 的大小比例默认为 8:1，保证了内存的利用率达到 90 %。如果每次回收有多于 10% 的对象存活，那么一块 Survivor 空间就不够用了，此时需要依赖于老年代进行分配担保，也就是借用老年代的空间。 IBM研究表明，98%的对象都是朝生夕死， 分配担保：存活对象超过了survivor的内存，通过分配担保机直接进入老年代一个需要理解的细节在交流过程中有一个细节，没有掌握清楚，记录在下面问题：分代垃圾回收中，既然复制算法效率高，速度快，为什么老年代没有采取复制算法？ 复制算法适用的场景是大多数对象都有一个很短的生存期，所以复制时会有少量的对象被复制到survivor区；老年代正好与此相反，大多数对象都有一个比较长的生存期，所以会造成每一次回收都必须复制大量的对象，效率必然不高； 最根本的缺陷，复制算法在垃圾回收时，如果存活的对象所占的内存大于survivor区域，此时年轻代的解决方案是用老年代的内存做分配担保；如果复制算法用在了老年代垃圾回收上，因为老年代对象的生存期长，很可能会发生survivor区域内存不足，此时却没有任何区域给老年代做分配担保；]]></content>
      <categories>
        <category>Interview</category>
      </categories>
      <tags>
        <tag>Interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[便利蜂技术面总结]]></title>
    <url>%2F2018%2F04%2F01%2FbianLiFeng%2F</url>
    <content type="text"><![CDATA[3.30号，下午请假去北邮参加校园宣讲会，现场笔试，直接三道编程题； 笔试第一道：给出一个正整数数组，返回他能组成的最大整数。举个例子arr = {19, 9, 912} 他应该组成的最大数为9 912 19；就是说数组中每个元素都不可以分割开。思路：首先将整数数组转换为一个字符串数组，每个元素都是一个字符串；找到元素中长度最长的那个，在例子中就是长度为3的 “912”；将其他的元素用他本身的最后一个字符补充至长度3，得到 “912”, “999”, “199”;现在将元素排序得到 “999”,”912”,”199”;最后将补充的字符都删除，即可得到：9 912 199；代码很杂乱，不献丑了；方法二：用到了贪心策略，局部最优，推广到整体。引进一个比较器，自定义比较方式；例如，在自定义之前是按字典序排序，该字符串数组排序结果为：”19” “9” “912”，举例子，显然我们需要的不是”19” &lt; “9”，我们需要的是链接后“199”&lt;”919”的顺序,需要的是”9129” &lt; “9912”,类似于这种，就是说我们需要的是链接后的顺序，不要链接前的顺序。贪心策略的证明很难，暂时不会，代码暂时没有问题；代码：123456789101112131415161718192021222324252627package bianlifeng;import java.util.Arrays;import java.util.Comparator;public class improveSolution1 &#123; public static class MyComparator implements Comparator&lt;String&gt;&#123; @Override public int compare(String str1, String str2) &#123; return (str1+str2).compareTo(str2+str1); &#125; &#125; public static void main(String[] args) &#123; int[] arr = &#123;18,89,12,56,4,3,7&#125;; String[] strArr = new String[arr.length]; for(int i=0; i &lt; arr.length;i++) &#123; strArr[i] = arr[i] + &quot;&quot;; &#125; Arrays.sort(strArr); System.out.println(&quot;不用比较器：&quot; + Arrays.toString(strArr)); Arrays.sort(strArr, new MyComparator()); System.out.println(&quot;用比较器： &quot; + Arrays.toString(strArr)); for(int i = strArr.length-1 ; i &gt;= 0; i--) &#123; System.out.print(strArr[i]); &#125; &#125;&#125; 第二道：给出一个数组arr，长度为n;每一个元素代表存在的货币值，现在给出一个aim，求最少用几张货币能够组成这个整数aim，每张货币不限使用次数。 经典的动态规划问题，构建dp矩阵，n行aim+1列； 第一列：aim = 0,自然第一列都是0； 第一行，自然只有k倍的arr[0]的地方有对应的次数，举个例子，假设arr[0] = 2;那么aim 只会等于2，4，6，8,这个样子，也就是在aim等于2的倍数的地方分别初始化为1，2，3，4； 其他位置dp[i][j] = min(dp[i-1][j], dp[i][j-arr[i]] + 1); 要注意的几个点，其实初始化0的地方就可以不初始化了，本来就是0；初始化第一行包括后面的dp[i][j],都应该先初始化为max然后满足条件的地方初始化为对应的值，因为后续处理中有个比较要取小值，会有影响； 代码：123456789101112131415161718192021222324252627282930313233343536373839package bianlifeng;public class impoveSolution2 &#123; public static void main(String[] args) &#123; int[] arr = &#123; 1, 7, 8, 9, 20, 50 &#125;; int aim = 15; int res = minCoins(arr, aim); System.out.println(res); &#125; private static int minCoins(int[] arr, int aim) &#123; int n = arr.length; int[][] dp = new int[n][aim + 1]; int max = Integer.MAX_VALUE; // 初始化第一列 for (int i = 0; i &lt; n; i++) &#123; dp[i][0] = 0; &#125; // 初始化第一行 for (int j = 1; j &lt;= aim; j++) &#123; dp[0][j] = max; if (j &gt;= arr[0] &amp;&amp; dp[0][j - arr[0]] != max) &#123; dp[0][j] = dp[0][j - arr[0]] + 1; &#125; &#125; // 计算其他位置 int left = 0; for (int i = 1; i &lt; n; i++) &#123; for (int j = 1; j &lt;= aim; j++) &#123; left = max; if (j &gt;= arr[i] &amp;&amp; dp[i][j - arr[i]] != max) &#123; left = dp[i][j - arr[i]] + 1; &#125; dp[i][j] = Math.min(left, dp[i - 1][j]); &#125; &#125; return dp[n - 1][aim]; &#125;&#125; 第三道：LRU最近最常使用；详细：因为内存有限，所以维持一个使用频率最高的数字集。例如有序列4 2 3 4 1 2 3假设维持的物理块为3则有。 4进，维持数组 4 2进，维持2 4 3进，维持3 2 4 4进，维持4 3 2 1进，维持1 4 3 2进，维持2 1 4 3进，维持，3 2 1 现在需要你构造一个数据结构满足以上条件。 要有 get(key)若key存在则返回对应值。若不存在返回null 要有set（key, value），若key不存在，则直接保存，若key存在，更新value。不能使用语言中已经存在的实现类。 面试第一部分是java的面试官； HashMap的底层实现。HashMap内部维护了一个存储数据的Entry数组，HashMap采用链表解决冲突，每一个Entry本质上是一个单向链表。当准备添加一个key-value对时，首先通过hash(key)方法计算hash值，然后通过indexFor(hash,length)求该key-value对的存储位置，计算方法是先用hash&amp;0x7FFFFFFF后，再对length取模，这就保证每一个key-value对都能存入HashMap中，当计算出的位置相同时，由于存入位置是一个链表，则把这个key-value对插入链表头。 假设我自己定义HashMap，只用数组存储，当冲突的时候向后放，这样实现的根本性的缺陷是什么？ HashMap的扩展过程； HashMap内存储数据的Entry数组默认是16，如果没有对Entry扩容机制的话，当存储的数据一多，Entry内部的链表会很长，这就失去了HashMap的存储意义了。所以HasnMap内部有自己的扩容机制。HashMap内部有： 变量size，它记录HashMap的底层数组中已用槽的数量； 变量threshold，它是HashMap的阈值，用于判断是否需要调整HashMap的容量（threshold = 容量*加载因子） 变量DEFAULT_LOAD_FACTOR = 0.75f，默认加载因子为0.75 HashMap扩容的条件是：当size大于threshold时，对HashMap进行扩容 扩容是是新建了一个HashMap的底层数组，而后调用transfer方法，将就HashMap的全部元素添加到新的HashMap中（要重新计算元素在新的数组中的索引位置）。 很明显，扩容是一个相当耗时的操作，因为它需要重新计算这些元素在新的数组中的位置并进行复制处理。因此，我们在用HashMap的时，最好能提前预估下HashMap中元素的个数，这样有助于提高HashMap的性能。 HashMap共有四个构造方法。构造方法中提到了两个很重要的参数：初始容量和加载因子。这两个参数是影响HashMap性能的重要参数，其中容量表示哈希表中槽的数量（即哈希数组的长度），初始容量是创建哈希表时的容量（从构造函数中可以看出，如果不指明，则默认为16），加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 resize 操作（即扩容）。 下面说下加载因子，如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长）；如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了），对空间造成严重浪费。如果我们在构造方法中不指定，则系统默认加载因子为0.75，这是一个比较理想的值，一般情况下我们是无需修改的。 另外，无论我们指定的容量为多少，构造方法都会将实际容量设为不小于指定容量的2的次方的一个数，且最大值不能超过2的30次方]]></content>
      <categories>
        <category>Interview</category>
      </categories>
      <tags>
        <tag>Interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志易技术面总结]]></title>
    <url>%2F2018%2F03%2F29%2FRiZhiYi%2F</url>
    <content type="text"><![CDATA[感谢日志易给的面试机会，感谢技术面大佬；最后厚着脸皮要了个评语：基础知识还行，但是不够熟练; 笔试这一部分的话，回忆几个比较重要的（我回答错）点。 字符串初始化与 == 比较题目：12345String str1 = &quot;hello&quot;;String str2 = &quot;he&quot; + new String(&quot;llo&quot;);String str3 = &quot;he&quot; + &quot;llo&quot;;System.out.println(str1 == str2);System.out.println(str1 == str3); 结果：false true 自增与等于结合题目：123int i = 1;int y = ++i;System.out.println(i + &quot; &quot; + y); 结果: 2 2 不同修饰符，修饰后能引用的范围：public: 同一类，同一包，子孙类，不同包；protect: 同一类，同一包，子孙类；default: 同一类，同一包；private: 同一类； 原生类与引用数据类型；原生类就是指java的8种基本类型，byte short int long float double char boolean；引用数据类型：数组，类，接口； 进程与线程的区别进程：是系统资源分配的基本单位，一个进程可以有多个线程；线程：是CPU调度的基本类型，线程不拥有系统资源，可以访问隶属进程的资源； 进程间的通信 管道（普通管道单向传递用在父子进程间，流管道可以双向传递用在父子进程间，命名管道单向传递可以用在不同进程间） 消息队列（克服了管道只能承载无格式字节流以及缓冲区受限） 套接字（可以用在不同机器的进程之间通信）； 共享内存：操作系统建立一块共享内存，映射到各个进程的地址，各个进程可以对这块共享内存进行读写，这是进程间通信的最快方式； 线程间的通信线程间的通信主要目的是用于线程同步 锁机制：互斥锁，读写锁，条件变量等； 信号量机制： 无名线程信号量，有名先线程信号量； 信号机制：类似与进程间的信号处理； HashMap的底层实现，哈希表的结构，冲突怎么解决的？内存模型与垃圾回收设计模式的应用场景数据结构题目一：给出一个n,代表有从1到n的数字[1,2,3,··· n]，问可以构成多少种二叉搜索树？一开始的想法是直接递归构造，时间复杂度是指数上升；后来想法是找规律：先看例子： n = 1, 有一个元素,可以构成一个二叉搜索树,左右都没有元素，总数量 = 左子树数量 右子树数量，记为f(1) = f(0) f(0) = 1，这儿可以将f(0)初始化为1; n = 2, 1做根，那么左子树没有元素记为f(0),右子树有一个元素记为f(1), 2做根，左子树有一个元素，记为f(1),右子树没有元素记为f(0);总共：f(2) = f(0) f(1) + f(1) f(0) = 2; n = 3, 1做根，数量 = f(0) f(2), 2做根 数量 = f(1) f(1), 3做根， 数量 = f(2) f(0);总共 f(3) = f(0) f(2) + f(1) f(1) + f(2) f(0) = 5;可以看出f(n),依赖与f(0)到f(n-1),换句话说可以有前面的n-1项推导出第n项；分析关系表达式： 记h(k)为以k为根可以生成的二叉搜索树数量； 当以k为根时，他的左子树为[1,2 ··· k-1]构成，也就是左子树有k-1个元素构成，这个就可以记为f(k-1)； 右子树为[k+1 ··· n]构成，也就是右子树有n-k个元素构成，这个可以记为f(n-k)； 那么h(k) = f(k-1) * f(n-k); 要记得k的范围可以从1到n; 整合以上规律可得到：有n个元素的二叉搜索树的数量；f(n) = h(1)+h(2)+···+h(n) = ∑ h(k) ,0 &lt; k &lt;= n; 又因为h(k) = f(k-1) f(n-k)得到：f(n) = ∑ f(k-1) f(n-k); 0 &lt; k &lt;= n; 代码：输入n，输出可以构造出的二叉搜索树的数量； 时间复杂度O(n^3)；1234567891011 private static int BSCount(int n) &#123; int[] res = new int[n+1]; res[0] = 1; for(int i = 1; i&lt;=n; i++) &#123; for(int k=1; k&lt;=i; k++) &#123; res[i] += res[k-1] * res[i-k];// System.out.println(i + &quot; k:&quot; + k +&quot; &quot; + res[i]); &#125; &#125; return res[res.length-1]; &#125; 题目二：给出一个文本，每行一个单词，10000行以上，将类似与”good”与”godo”与”doog”，也就是单词所包含的字母出现次数一样，这类单词都定义同义词，现在输入一个单词，要求返回该单词的所有同义词；要求查询速度要快。 解析：这个方法是面试官告诉我的，我自己拿一套就不提了，太傻。首先我们理解诉求，要查询快，当然用哈希表；那么什么东西做key，什么东西做value;因为要返回输入单词的所有同义词，那么value可以是一个字符串类型的数组；我们将所有单个单词都进行排序，那么同义词都长一个样，我们就拿这个排序后的单词做key; 最难的部分解决了，说说流程；我们需要一个HashMap&lt;String, ArrayList&gt;,value是一个列表；每次从文本读入一个单词，排序后做key,然后存入HashMap；如果key存在，那么获得value的指向的ArrayList,将原本的单词，存入个ArrayList;如果key不存在，那么我们需要新建一个Arraylist,然后将读入的单词存入ArrayList,最后将ArrayList存入HashMap；查询时记得，将输入的单词排序做key去查value; 代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546package rizhiyi;import java.io.BufferedReader;import java.io.File;import java.io.FileReader;import java.io.IOException;import java.util.ArrayList;import java.util.Arrays;import java.util.HashMap;import java.util.Scanner;public class Synonym &#123; public static void main(String[] args) throws Exception &#123; Scanner sc = new Scanner(System.in); String str = sc.next(); sc.close(); File file = new File(&quot;H:\\java\\example.txt&quot;); ArrayList&lt;String&gt; arr = countSameword(str, file); System.out.println(arr.toString()); &#125; private static ArrayList&lt;String&gt; countSameword(String s1, File file) throws IOException &#123; BufferedReader bfr = new BufferedReader(new FileReader(file)); String line = null; HashMap&lt;String, ArrayList&lt;String&gt;&gt; map = new HashMap&lt;String, ArrayList&lt;String&gt;&gt;(); while((line = bfr.readLine()) != null) &#123; char[] ch = line.toCharArray(); Arrays.sort(ch); String str = new String(ch); if(map.get(str) != null) &#123; ArrayList&lt;String&gt; value = map.get(str); value.add(line); &#125;else &#123; ArrayList&lt;String&gt; arr = new ArrayList&lt;String&gt;(); arr.add(line); map.put(str, arr); &#125; &#125; bfr.close(); char[] chRes = s1.toCharArray(); Arrays.sort(chRes); s1 = new String(chRes); return map.get(s1); &#125;&#125; 题目三：给出2^200次最简单的算法，并且证明算法是最简单的。 这个问题我还没有思路解决在数学上证明方法是最简单的；但是在书上找到了一种相对简单的方法； 先将200转为二进制表示法，11001000； 2^200 = 2^128 * 2^64 * 2^8;在这个过程中我们只需要计算2^1, 2^2, 2^4, 2^8, 2^16, 2^32, 2^64, 2^128,也就是二进制有多少位，我们就需要计算多少次； 在计算最后结果时，只需要二进制对应是1的地方累乘，计算对应0的地方不需要累乘； 没有考虑大数 代码：123456789101112131415161718192021222324package rizhiyi;// 求2^50最快捷的方式public class bigPower &#123; public static void main(String[] args) &#123; int n = 50; String str = Integer.toBinaryString(n); System.out.println(&quot;二进制表示： &quot; + str); char[] ch = str.toCharArray(); long[] res = new long[str.length() + 1]; res[0] = 1; res[1] = 2; long ans = 1; for (int i = 2; i &lt;= str.length(); i++) &#123; res[i] = res[i - 1] * res[i - 1]; &#125; for (int j = ch.length - 1; j &gt;= 0; j--) &#123; if (ch[j] == &apos;1&apos;) &#123; ans *= res[res.length - 1 - j]; &#125; &#125; System.out.println(ans); &#125;&#125;]]></content>
      <categories>
        <category>Interview</category>
      </categories>
      <tags>
        <tag>Interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vivo技术面试总结-大数据开发岗]]></title>
    <url>%2F2018%2F03%2F20%2FvivoInterview%2F</url>
    <content type="text"><![CDATA[过了笔试，很幸运的得到了面试机会，背着我的小书包，挤着地铁就去了西土城面试； 自我介绍这部分就不多说了，我说的也不好，大致介绍了一下个人的基本情况； 数据结构 给100W个区间，不重叠。给出一个数N，求这个数在那个区间。空间复杂度要求我看到了，但估计我太菜了，人家没问;给出想法： 把所有区间的右边界，用一个数组存起来; 给数组排序; 二分查找，找到最后一个区间，返回右边界。这就是所在区间的右边界;要求写出大概的代码：现场我没写出来，时间有点紧；12345678910111213141516171819202122232425package vivo;import java.util.Arrays;public class search100w &#123; public static void main(String[] args) &#123; int n = 89; int[] arr = &#123;5,10,20,30,40,60,50,55,100,95,90&#125;; Arrays.sort(arr); System.out.println(Arrays.toString(arr)); int index = binarySearch(arr,n); System.out.println(&quot;区间的右边界是： &quot; + arr[index]); &#125; private static int binarySearch(int[] arr, int n) &#123; int low = 0; int heigh = arr.length-1; while(low &lt; heigh) &#123; int mid = (low+heigh)/2; if(n &lt; arr[mid]) heigh = mid - 1; else low = mid + 1; System.out.println(low +&quot; &quot;+ heigh); &#125; return heigh+1; &#125;&#125; JAVA 接口，抽象类，以及实现，问能不能写一个小栗子，说明一下区别；优缺点，大概这个样子。我大致说了一下，但是没写出小例子;大致区别：接口支持多实现，类不支持多继承，这样接口更利于扩展；实现接口，必须实现接口中的所有方法，继承抽象类，却不一定要实现他的所有方法；接口中成员变量都必须被public static final,成员函数都必须被public abstract修饰，抽象类中可以用public protected default abstract;抽象类的方法可以有默认实现，但是接口不可以;抽象类的速度要比接口快;添加新方法：接口，要添加就必须修改实现类，抽象函数却可以有默认的实现; HashMap与HashTable的区别：底层数据结果哈希表，特点和HashMap是一样的;Hashtable线程安全集合，运行速度慢;HashMap线程不安全的集合，运行速度快;Hashtable命运和Vector是一样的，从JDK1.2开始，被更先进的HashMap取代;HashMap允许存储nu11值，nu11键;Hashtable 不允许存储nu11值，nu11键;源码比较：http://blog.csdn.net/ns_code/article/details/36034955 HashMap存储，两个键值对中key如果哈希值相同是怎么存储的?哈希值相同，但内容不相同，采用桶存储;哈希值相同，equals()比较内容也相同的话，就不存储，因为这个情况下，key相等，不允许这种情况发生;扩充：HashMap和和Hashtable都是基于哈希表存储数据的，具体就是：内部维护了一个存储数据的Entry数组，HashMap采用链表解决冲突，每一个Entry本质上是一个单向链表。当准备添加一个key-value对时，首先通过hash(key)方法计算hash值，然后通过indexFor(hash,length)求该key-value对的存储位置，计算方法是先用hash&amp;0x7FFFFFFF后，再对length取模，这就保证每一个key-value对都能存入HashMap中，当计算出的位置相同时，由于存入位置是一个链表，则把这个key-value对插入链表头。 StringBuffer与StringBuilder的比较：前者线程安全，但是速度慢，后者线程不安全，但速度快。StringBuilder类提供与StringBuffer 相同的方法。 Linux说自己用过的Linux命令：ls ll la, cat more tail, cd, vi, mkdir touch mv cp scp ftp, chomd, cut sed awk;基本就是这样了，这部分平时用就肯定脱口而出，都没问怎么用的，感觉会不会一眼就看得出来; 自己介绍一下了解的hadoop hadoop三个组件，HDFS, MapReduce,Yarn; HDFS: 分布式存储框架，namenode, secondnamenode, dataname, 元数据，持久化的命名空间镜像文件，编辑日志。HA高可用; MapReduce: map-shuffle-reduce ,shuffle: combiner, partition, sort, copy, sort; Yarn: Resouce Manage, Application Manage, Namenode Manage;这一部分我自己了解的比较清楚，名词都大致有个解释。 地图的导航功能背后是怎么存储数据的额，他又是怎么做到精确导航的；没答上来,人家就没扩展直接跳过了； 笔试的一个题 寻找最长的重叠字符串，abcabc这种定义位重叠字符串123456789101112131415161718192021222324252627package vivo;import java.util.Arrays;public class FindRepeat &#123; public static void main(String[] args) &#123; String str = &quot;abcdefefvivovivoghijghijk&quot;; char[] ch = str.toCharArray(); System.out.println(Arrays.toString(ch)); String childStr = null; int maxLength = 0; String res = null; for(int i=0; i&lt;ch.length; i++) &#123; for(int j=i+1; j&lt;=ch.length; j++) &#123; childStr = new String(ch, i ,j-i);// System.out.println(childStr); int first = str.indexOf(childStr); int last = str.lastIndexOf(childStr); if(first != last &amp;&amp; childStr.length()&gt;maxLength) &#123; res = childStr;// System.out.println(&quot;res :&quot; +res); maxLength = childStr.length(); &#125; &#125; &#125; System.out.println(res); &#125;&#125;]]></content>
      <categories>
        <category>Interview</category>
      </categories>
      <tags>
        <tag>Interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Get与Post的区别]]></title>
    <url>%2F2017%2F12%2F06%2Fdiff-get-post%2F</url>
    <content type="text"><![CDATA[先说结论：他们的本质区别在语义上，Get是获取资源，Post是处理资源。由此延伸出来的区别就是：Get是安全的，幂等的，可缓存的。Post不安全的，不幂等的，不可缓存的。安全的：是指他不对服务器资源做任何修改，是一个只读请求，对服务器来说这个请求就是安全的。幂等的：执行一次和多次，效果相同，不对服务器上资源做修改。可缓存：这个我不知道怎么解释。至此Get与Post的区别就完了。以下都是自己的理解：Get与Post本质上是http协议中的请求方法，协议是人定的，但是不是所有人有遵守协议就不一定了。如果有人不遵守协议规定的语义，例如使用Get请求去处理资源，用Post请求去请求资源，也就是说Get与Post请求可以去完成对方做的事，只要对应的语法是正确的，服务器就会作出回应。但是这样子做势必会有问题。所表现出来的一些问题，被一部分资料说成了Get与Post的区别。那么人们常常说的Get与Post的哪些区别是什么？我理解这只是他们表现出来的现象上的区别。例如W3c所罗列的： Get Post 后退按钮/刷新 无害 数据会被重新提交（浏览器应该告知用户数据会被重新提交）。 书签 可收藏为书签 不可收藏为书签 缓存 能被缓存 不能缓存 编码类型 application/x-www-form-urlencoded application/x-www-form-urlencoded 或 multipart/form-data。为二进制数据使用多重编码。 历史 参数保留在浏览器历史中。 参数不会保存在浏览器历史中。 对数据长度的限制 是的。当发送数据时，Get 方法向 URL 添加数据；URL 的长度是受限制的（URL 的最大长度是 2048 个字符）。 无限制。 对数据类型的限制 只允许 ASCII 字符。 没有限制。也允许二进制数据。 安全性 与 Post 相比，Get 的安全性较差，因为所发送的数据是 URL 的一部分。发送密码或其他敏感信息时绝不要使用Get ！ Post 比 Get 更安全，因为参数不会被保存在浏览器历史或 web 服务器日志中。 可见性 数据在 URL 中对所有人都是可见的。 数据不会显示在 URL 中。 注意！！！ 长度限制，是浏览器对URL长度有限制，而Get方法本身对数据长度是没有限制的。 安全性，这个现象是存在的，但是个人认为是不按照语义使用相应的请求造成的，Get请求本身就不应该带这些数据与服务器交互。详细参考资料：现象：点击查看W3c本质：点击查看博客]]></content>
      <categories>
        <category>Http</category>
      </categories>
      <tags>
        <tag>Http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive常用命令]]></title>
    <url>%2F2017%2F12%2F01%2Fstart-hive%2F</url>
    <content type="text"><![CDATA[Hive 常见命令，老大说学完就给我点权限。你看看，这孩子又在写Bug了。 Hive常见命令 显示所有数据库show databases; 指定使用某个数据库use database_name; 显示所有表show tables; 查询表结构desc table_name; 显示表的详细信息describe extended table_name; 显示分区信息show partitions; HIVEQL创建表123456789101112create table if not exists test_klh (name string comment &apos;person name&apos;, age int comment &apos;person age&apos;,sex string comment &apos;person sex&apos;)comment &apos;person&apos;row format delimited# 列分隔符（字段）fields terminated by &apos;|&apos;# 行分隔符lines terminated by &apos;\n&apos;# 本地文件的格式stored as textfile; 表结构复制，不会复制数据create table if not exists test_klh2 like test_klh; 加载数据(将本地数据加载到Hive表)load data local inpath &#39;/kehuduan02/bonc_guanjianji/bonc_klh/test&#39; overwrite[可省] into table test_klh; 加载数据同时创建分区load data local inpath &#39;path/path&#39; overwrite into table table_name partition (sex, age); 导出数据insert overwrite local directory &#39;path&#39; select * from table_name where ** 通过查询插入数据insert into/overwrite table test_klh select * from test_klh where name = &quot;mike&quot;; 单个查询语句中创建表并加载数据create table test_klh1 as select * from test_klh where name = &quot;mike&quot; ; 删除表drop table if exitst table_name case … when … then …1234567select name, sex,case when test_klh.sex &lt; 22 then &apos;younger&apos; when test_klh.sex &gt; 20 and test_klh.sex &lt; 23 then &apos;mid&apos;when test_klh.sex &gt; 22 then &apos;bigger&apos;else &apos;bingo&apos;end as bracket from test_klh; 上面这些学完真的就可以去练练手了。 两个很实用的命令下面介绍；两个很实用的命令 hive -e: 在 linux 命令行中执行 hive语句，其实就是用hive来解析hive -e 后面的语句。12345678[root@cloud1 hive-0.13.1]# bin/hive -e &apos;select * from default.student&apos; 15/10/18 06:55:27 WARN conf.HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect. Use hive.hmshandler.retry.* instead Logging initialized using configuration in jar:file:/opt/hive-0.13.1/lib/hive-common-0.13.1.jar!/hive-log4j.properties OK 1 jhon 2 mike 3 jack Time taken: 21.298 seconds, Fetched: 3 row(s) hive -f：在linux 命令行中执行 hive的.sql文件，其实就是用hive解析一个.sql 文件。1234567891011[root@cloud1 hive-0.13.1]# touch test.sql [root@cloud1 hive-0.13.1]# vi test.sql select * from default.student ; [root@cloud1 hive-0.13.1]# hive -f /opt/hive-0.13.1/test.sql 15/10/18 07:00:12 WARN conf.HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect. Use hive.hmshandler.retry.* instead Logging initialized using configuration in jar:file:/opt/hive-0.13.1/lib/hive-common-0.13.1.jar!/hive-log4j.properties OK 1 jhon 2 mike 3 jack Time taken: 13.048 seconds, Fetched: 3 row(s) 注意：我一般都把注释写在代码的上一行。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从Hive导出数据到Oracle数据库--Sqoop]]></title>
    <url>%2F2017%2F12%2F01%2FHive-Oracle%2F</url>
    <content type="text"><![CDATA[实习老大让我把Hive中的数据导入Oracle数据库。摸索成功后记录如下：首先解释一下各行代码：123456789101112131415161718192021222324sqoop export# 指定要从Hive中导出的表--table TABLE_NAME # host_ip:导入oracle库所在的ip:导入的数据库--connect jdbc:oracle:thin:@HOST_IP:DATABASE_NAME # oracle用户账号--username USERNAME# oracle用户密码--password PASSWORD # hive表数据文件在hdfs上的路径--export-dir /user/hive/test/TABLE_NAME# 指定表的列名，必须指定 --columns ID,data_date,data_type,c1,c2,c3 # 列分隔符(根据hive的表结构定义指定分隔符)--input-fields-terminated-by &apos;\001&apos;# 行分隔符--input-lines-terminated-by &apos;\n&apos; # 如果hive表中存在null字段，则需要添加参数，否则无法导入--input-null-string &apos;\\N&apos; --input-null-non-string &apos;\\N&apos; 不知道表存在哪儿了: show create table table_name;然后来个小栗子：123456789101112sqoop export \--connect jdbc:oracle:thin:@172.12.12.102:orcl \--username test \--password kong \--table table_abc \--export-dir /user/hive/warehouse/bonc_gjj.db/table_abc \# 注意，这一行columns不能有多余的空格，否则会报错。--columns zzjgdm,jgmc,jglx,jjlx,frdbhfzr,xzqhdm,yzbm,tzgb,hbzl,jgdz,dh,yxqzfrq,zczj,njq0,fzrq,zzzt,pzwhhzch,bfdw,lastdate,id,dir_id,dir_ver,dir_ver_serail_num,addtime,updatetime,edituser_id,edituser,editdept_id,editdept,inserttype,is_valid,audit_status,pk_md5,sys_encrypt \--input-fields-terminated-by &apos;\001&apos; \--input-lines-terminated-by &apos;\n&apos; \--input-null-string &quot;\\\\N&quot; \--input-null-non-string &quot;\\\\N&quot; 最后，表那么多，总不能一张一张手动导入吧，那就来个脚本吧。hh脚本奉上，简单的要死，看看就会：123456789101112131415161718192021222324252627#!/bin/bash a=0;b=1;# ``这两个反斜点，就是说里面这是一个变量，我的have_data_table_name是一个文件，里面存的是一堆表名。# cat file_name，自己试试什么效果。for 开始循环表名。for table_name in `cat ./have_data_table_name` do a=`expr $a + $b` echo &quot;表名：$table_name,计数：$a&quot;; echo &quot;开始导入数据！&quot; # 这一行就厉害了，简单来说就是取出一张表的所有列名，每个列名后加个逗号，然后去掉最后一个逗号，存在col这个变量中。 col=`hive -e &quot;desc database_name.$&#123;table_name&#125;&quot;|sed &apos;1d&apos;|awk &apos;&#123;printf $1&quot;,&quot;&#125;&apos;|sed &apos;s/,$/\n/g&apos;`sqoop export \--connect jdbc:oracle:thin:@172.12.12.102:1521:orcl \--username test \--password kong \--table $&#123;table_name&#125; \--export-dir /user/hive/warehouse/database_name.db/$&#123;table_name&#125; \--columns $&#123;col&#125; \--input-fields-terminated-by &apos;\001&apos; \--input-lines-terminated-by &apos;\n&apos; \--input-null-string &quot;\\\\N&quot; \--input-null-non-string &quot;\\\\N&quot; echo &quot;第$&#123;a&#125;张表导入完毕！&quot;;done 最后你就可以一边喝茶，一边看电脑跑脚本，数据就这么被导入了。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伪分布式hadoop2.8.0安装与环境搭建]]></title>
    <url>%2F2017%2F11%2F30%2Fhadoop2-8-0_built%2F</url>
    <content type="text"><![CDATA[详细教程这儿有篇宝典，简单有效，相见恨晚：点击打开宝典 安装SSH，配置SSH的无密码登录。 记得先更新一下APT：sudo apt-get update 安装个Vim ：sudo apt-get install vim 安装SSH服务：sudo apt-get install openssh-server 安装后登陆一下本机： ssh localhost 这时候是需要密码的，然后退出准备配置无密码登录： exit 开始: cd ~./ssh ssh-keygen -t rsa #一直回车就行 cat ./id_rsa.pub &gt;&gt; ./authorized_keys #加入授权环境配置知识点。 环境变量临时设置，直接在终端输入，属于临时设置。export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export PATH=$JAVA_HOME/bin:$PATH 当前用户的全局设置打开~/.bashrc，添加行：export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export PATH=$JAVA_HOME/bin:$PATH使生效source ~/.bashrc 所有用户的全局设置$ sudo vim /etc/profileexport JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export PATH=$JAVA_HOME/bin:$PATH使生效source /etc/profile个人建议环境变量用第三种方式配置。 我的错误：配置环境时在这句中 ： export PATH=$JAVA_HOME/bin: 中没有写最后的$PATH.后果就是所有的命令都无法正常使用了。怎么办呢？命令还在只是计算机没有办法自己找到。那就我们代劳。解决方法：写命令的绝对路径，举个例子，假设 vim 这个命令在 /bin 下那么使用 vim 就要写 ./bin/vim 然后重新编辑环境变量。jps 后没有namenode解决办法：点击转到解决办法hadoop用户最好直接新建 hadoop 用户，不要轻易尝试将当前用户名改为hadoop.这是个大坑，亲测。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客搭建过程]]></title>
    <url>%2F2017%2F11%2F22%2FBlog_built%2F</url>
    <content type="text"><![CDATA[个人理解：简单说一下个人对搭建理解： github相当于是服务器 hexo替生成漂亮的页面 通过hexo命令将生成的页面部署（就是上传）到github, github替你将这些页面保存起来。有人访问你的博客，github 就自己发给他。 搭建过程： 博客搭建详细教程：神秘链接 next主题配置教程：神秘链接 搭建过程有上面两个教程就足够了，但是也不妨看看文档。 Hexo文档：神秘文档 个人遇到的问题： hexo 无法安装，就是命令敲进去了没反应，或者安装一半卡死了。 正常安装Hexo时卡死解决办法： npm install -g cnpm –registry=https://registry.npm.taobao.orgcnpm install hexo-cli -g 结束语： 新建文章： hexo new title 在本地新建了 .md 文件 生成静态页面： hexo generate 在本地生成 html+css+js 文件 清除生成内容： hexo clean 清除本地生成的文件 部署Hexo：hexo deploy 将本地文件部署到 GitHub 最后说一句，了解一下超简单的几个MarkDown语法就可以轻松编辑 .md 文件。就是在写文章啦。 超简单的MarkDown写法]]></content>
      <categories>
        <category>Blog相关</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>next</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MarkDown常用语法（转载）]]></title>
    <url>%2F2017%2F11%2F20%2FMarkDown%2F</url>
    <content type="text"><![CDATA[标题这是最为常用的格式，在平时常用的的文本编辑器中大多是这样实现的：输入文本、选中文本、设置标题格式。而在 Markdown 中，你只需要在文本前面加上 # 即可，同理、你还可以增加二级标题、三级标题、四级标题、五级标题和六级标题，总共六级，只需要增加 # 即可，标题字号相应降低。例如： 一级标题二级标题三级标题四级标题五级标题六级标题注：# 和「一级标题」之间建议保留一个字符的空格，这是最标准的 Markdown 写法。 列表列表格式也很常用，在 Markdown 中，你只需要在文字前面加上 - 就可以了，例如： 文本1 文本2 文本3如果你希望有序列表，也可以在文字前面加上 1. 2. 3. 就可以了，例如： 文本1 文本2 文本3注：-、1.和文本之间要保留一个字符的空格。 链接和图片在 Markdown 中，插入链接不需要其他按钮，你只需要使用 [显示文本](链接地址) 这样的语法即可，例如：[简书](http://www.jianshu.com) 点击转到简书 在 Markdown 中，插入图片不需要其他按钮，你只需要使用 ![](图片链接地址) 这样的语法即可，例如：![](http://upload-images.jianshu.io/upload_images/259-0ad0d0bfc1c608b6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)注：插入图片的语法和链接的语法很像，只是前面多了一个 ！。 引用在我们写作的时候经常需要引用他人的文字，这个时候引用这个格式就很有必要了，在 Markdown 中，你只需要在你希望引用的文字前面加上 &gt; 就好了，例如：&gt; 一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 一盏灯， 一片昏黄； 一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。注：&gt; 和文本之间要保留一个字符的空格。 粗体和斜体Markdown 的粗体和斜体也非常简单，用两个 包含一段文本就是粗体的语法，用一个 包含一段文本就是斜体的语法。*一盏灯*， 一片昏黄；**一简书**， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。最终显示的就是下文，其中「一盏灯」是斜体，「一简书」是粗体：一盏灯， 一片昏黄；一简书， 一杯淡茶。 守着那一份淡定， 品读属于自己的寂寞。 保持淡定， 才能欣赏到最美丽的风景！ 保持淡定， 人生从此不再寂寞。 代码引用需要引用代码时，如果引用的语句只有一段，不分行，可以用反斜点将语句包起来。&gt; `如果引用的语句为多行，可以将三个反斜点置于这段代码的首行和末行。12假如这是引用的多行代码假如这是引用的多行代码 表格相关代码：Tables | Are | Cool |1234| ------------- |:-------------:| -----:|| col 3 is | right-aligned | $1600 || col 2 is | centered | $12 || zebra stripes | are neat | $1 | 效果：| Tables | Are | Cool || ————- |:————-:| —–:|| col 3 is | right-aligned | $1600 || col 2 is | centered | $12 || zebra stripes | are neat | $1 | 作者：简书链接：http://www.jianshu.com/p/q81RER來源：简书]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人生十之八九不如意]]></title>
    <url>%2F1897%2F04%2F01%2FlifeNum04%2F</url>
    <content type="text"><![CDATA[我现在严重怀疑我上辈子大概是佛祖吧，转世投胎到这辈子，还残留着一点点预言或者先知的能力。一言成谶的事儿没少干，我都想毁了我这张臭嘴。简而言之就是在上个周末跟朋友说，最近互联网情况不太乐观，下次见到我，说不定我就被裁了。嗯！应验了，周三通知被裁了，周五离职手续都办完，走人啦！ 离职初体验这段应该在上个周末来写，可能情绪最为饱满，骂人也骂的酣畅。其实被裁员这事儿说大也大，说小，也算不上什么毁天灭地；我是苦出身，现在的工作得来也不算容易，所以期望稳稳妥妥的进行换工作，现在这个间隔期等于是失业状态。一开始确实很难以接受被裁员这回事。因为这从根本上来说，是公司对你个人的不认同。就这一条就足够你怀疑自己。但是结合实际情况来看，不是个例，一起入职的小伙伴一起去办了离职。公司放弃了一批年轻人。 离职再体验等到渡过了最难过，情绪最繁杂的那几天。应该在这个时间段再回味一遍，发生了什么。自己的应对与处理是不是合适。一定有可以改正的错误。今天算是情绪与逻辑都回归的第一天，度过这段繁杂的情绪肯定要浪费掉一段时间。幸亏我不是一个人住，有很多好朋友，也用开玩笑的方式，笑骂了这些屁事儿； 回看过往的一年承认自己的不足 与同组同事相比较，技术能力上来说，确实相差较多。 参加工作后确实有一段时间大概两个多月，是不够用心去学习的，放松了自己。这确实导致了成长速度缓慢。 讲故事能力。 为自己不足找借口，说原因 的确我原来不是写Java的，也不搞Spring。转到这个方向，半年时间，现在可以说是用起来了这一套工具，也熟悉了开发的整个流程；自己负责一个模块去迭代的话也可以。但我的这些同事不是正当打的两三年选手，就是经验丰富的老手。确实很难相提并论。如果要博取一丝的机会，现在的成长速度的确不够；回过头看这一点，距离自我要求确实不达标。还是对自己放松了要求。 持续进步，持续努力真的是程序员必备的习惯。搞那种突破性的学习，长期来看，收益确实比较低下。每天进步一点点就可以，每天可以抽一个小时给自己学学新东西，技术积累也急不得。摆平心态，继续努力！ 所谓讲故事能力，主要体现在一些分享或者工作总结上，的确大多数的程序员，包括我，一提到这件事情态度就是，技术学好，技术牛逼不就行了！但是啊，技术牛逼不到一定的地步，就是没有牛逼到同事都夸赞你的这个地步。还是不要抱有这种态度为好！我们太多的人只能是普普通通，平平凡凡的程序员。你的老板是没有渠道去看到你为具体的事情做了多少努力的。唯一了解你动态，了解你做了什么事情的时候就是这些个不大不小的工作总结。所以工作总结这件事情是相当重要的一件事情。务必把自己做的事情讲清楚，一步步的还要成为一个故事。 千言万语，还是保持平常心，认真努力的做事，创造价值。 一些细节我的小心谨慎就像作家隔壁老王说的，她应该对贫穷恨之入骨。没错，从贫穷困境里走出的孩子，我见过一些内心是怀有恨意的。这是生活给他们的烙印。因为不被善待，他们对世界是谨慎而充满敌意的。他们也会把恨转变成力量，也可能读到很高学位，实现很大成就。可他们人格里的自卑阴影，和别人沟通的障碍，可能会终身给他们带来影响。 我开始发现在和别人沟通中，自己性格中那些小心谨慎而充满敌意。当对方表达出惊讶夹杂着不满之后，又迅速觉得是自己理亏，迅速道歉。然后对方变得更加难以理解。 小心敬慎是因为自身不具备抗风险能力，]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给未毕业的你们一些小小的建议]]></title>
    <url>%2F1896%2F12%2F23%2FlifeNum03%2F</url>
    <content type="text"><![CDATA[最近大学辅导员老师让写一点工作相关的事情，写写找工作的经验，大学应该怎么过等一系列我都没怎么想明白的问题；开玩笑的，还是有一点点感悟的。扯淡完毕，正好就着小学弟问的一些问题，写一点自己的想法，看看就行，各位自己的路始终还是得自己走。 问题一览 你认为你的职业最大的阻力在哪？ 专业知识对从事该职业重不重要？ 平常在工作方面,你每天都做些什么工作?满足你之前的预期吗？ 你的工作的薪资水平如何，有津贴和福利吗？ 我想问一下从事这份工作的人的发展前景或晋升机会如何? 现在公司工作对职员有哪些要求？ 你为什么选择现在的公司？ 学长工作之余 会做些什么(提升自我or放松)先说说我看完这些问题的感受，说实话这些问题我认为只有少数几个问题问到了点上，大多数问题纵使我口吐莲花般的回答了，也不见得对你们未来有什么帮助。但是我很能理解为什么你们会问出这样的一些问题。因为提问者才大二，要是我在大二估计也会这样问。因为还没真的到被逼迫找工作的时候，想的场景自然没那么丰富。我会以问题为基准回答然后在拓展一些，希望对你们有所帮助。 你认为你的职业最大的阻力在哪？这个问题只能缩小范围来谈，就固定在从学生到参加工作这一个阶段来谈，才能回答的比较好。对于我来说最大的阻力就是技术深度不够，基础知识略显单薄，这也是在我众多次面试中听到的最多的评价。单从课堂上学到的那些知识是不足以应付我现在的工作的。所以打算在大学本科毕业就工作的同学们，还是要在课余时间，投入时间去学习更多的专业知识。 专业知识对从事该职业重不重要？首先说我的职业：程序员 再说我的答案：专业知识很重要。专业知识决定你能不能找到工作，也决定了你能不能找到一份满意的工作。 最后说专业知识指的是什么?对于未来有志于加入程序员这一行业的同学来说在校期间做好以下三件事，毕业后找到一份满意的工作就不成问题。 第一：掌握一门编程语言，我校开的C++是基础，这个学的明白了，后面JAVA就学的比较容易了； 第二：掌握数据结构，这个不单指这门课程，更加重要的是算法，积极参加我校的ACM比赛。平时刷刷OJ的题目。蓝桥杯记得参加，数统院有你们学长拿过一等奖，不是我。 第三：工程能力，这个其实在学校如果不关注的话，基本就只剩计算机课设能涉及到了。其实可以自己尝试搭建个网站，这就囊括了很多知识了；以上三点，其实涉及到很多的知识点，其重要程度，依次序降低。因为基础的编程语言能力和算法能力这都是一点一点积累出来的。这是硬实力。而工程能力是可以突击学习的。如果要问我具体的一些知识点，推荐你们多看看牛客网。有很多不错的板块，例如面试经验分享，这一块有写的很详细的。还有一些公司的笔试题，可以了解一下，看看自己会不会；这些都是一手资料，看完自然就不再迷茫了，也自然知道自己该做什么事情了，早做准备，后续会轻松很多。牛客地址：https://www.nowcoder.com/顺便推荐一个知识库，这个基本囊括了你毕业之前要学的知识点，这个是一个研究生整理的，你要是能全部掌握，那你就可以挑公司了，而不再是被公司挑；地址： https://github.com/CyC2018/CS-Notes 平常在工作方面,你每天都做些什么工作?满足你之前的预期吗？现状满足我之前的预期，但会有更多的想法或者说追求； 每天的工作现阶段分两种情况，没有任务或者有任务； 没有任务就自己学习，因为自己欠缺的还多； 有任务就是想着怎么完成任务。完成任务一般分两步： 第一步思考这个任务怎么完成，也就是设计程序的大纲，想明白先干什么，后干什么；自己的想法能不能顺利的转换成代码；第二步就是写代码，写完代码，调试代码，跑的通，跑的顺畅； 多说一点，除了写代码也会有一些开会或者跟同事讨论怎么更好的完成开发； 你的工作的薪资水平如何，有津贴和福利吗？ 薪资水平也满足之前的预期； 每个公司都有不同的津贴和福利方案；但大概就是每个月有一定餐费和车费；我所在的公司每个月固定有400块的饭补，如果当天工作超过11个小时有30的晚餐费和70的打车费；福利：每个传统的节日公司会有小礼品赠送，大概价值100元的样子；茶水和咖啡不限量； 我想问一下从事这份工作的人的发展前景或晋升机会如何?任何行业发展前景都看个人努力，现在热门的行业未来不一定热门；但是计算机行业在可预见的未来依然是很重要的行业，可以放心大胆的加入这个行业；晋升机会，一年一次机会。晋升自然就要涨工资了；只要认真工作吗，机会还是很大的； 在现在的公司工作对职员有哪些要求？抛开品行，道德这些不谈，不是说不重要，是因为大多数人在这个方面都没有问题，所以是个正常人就行；对于做技术的人来说，技术到位，礼貌待人就可以了。 你为什么选择现在的公司？在我所有得到的机会中，现在公司的名声在外而且当时想做的事也很对口，待遇也好，所以就来了哇！扩展一点：都说面试是一个双向选择，其实是公司先选择了你，然后选择权才到你手里。假如你足够优秀，拿到了国内顶尖的那几家互联网公司的offer，那个时候选择才显得有意义，多数人在大多数时候其实是没得选的。因为最好的其实就那几家，能获得其中一家的青睐就去这家吧，不会吃亏。非要说怎么选择，考量的东西每个人的侧重点都会不同，但相同的地方其一是待遇，其二就是做的事情；前者你满意就行，后者你认同就行。 学长工作之余 会做些什么(提升自我or放松)现阶段工作之余就是放松，因为基本平均下来每个工作日工作时间大概都在十个小时，再加上早晚通勤就要花掉两个小时，剩下来给自己的时间其实不多。所以大部分时间还是选择放松。就我个人来说，我比较喜欢小说，各种类型的都看。周末跟朋友打打游戏。偶尔出去打打球。生活还是挺简单的。 我认为大学应该怎么过我认为大多数人解决一些以下几个问题，大学自然过得比较充实。 考研与工作选那个：如果考研也是考计算机的研究生（我相信信科大多数人都是这条路），那就没必要迷茫了。计算机相关的东西就一点一点学。上面在专业知识那一块提到的三个点。一个一个的解决吃透，考研与找工作在这三个点上一点都不冲突； 学什么，怎么学？大一大二，编程语言和数据结构，然后穿插着刷算法题，这样做两年基础绝对不会差；大三想明白走哪个方向，前端还是后端，然后学习对应方向的技术。怎么学，买书（或者去图书馆借，可以先借来看，是好书再买回来），买视频教程。学技术目标就是懂原理能上手；当然看书学的最快，但是入门还是推荐看视频，视频比书籍的好处就是可以实现手把手的教。书籍则侧重原理的讲解，往往会省略掉一些操作细节。这对新手入门并不友好； 对于下定决心要工作的同学，一定要出去实习。而且越早越好。 毕业季怎么找工作（或者实习）修炼好了功夫，找工作也需要主动出击。线上线下结合。 线上牛客网的招聘信息汇总最全，心里有底的同学挑着想去的公司投简历，心里没底的把对口的公司都投一遍。然后等待笔试面试就可以了。 线下9月份可以去总校跑一趟。也有很多机会。我的第一个offer就是在总校找的。有了第一个，必然会有更多个。 很重要的共识：心态要稳，秋招不满意，还有春招。期间隔着接近小半年时间，你经历了秋招，自然明白欠缺的是什么，疯狂的补回来就可以；你觉得面的非常好的公司不一定会要你，不是因为你不好，不要自我否定。这其中可能有很多的因素。坚定相信自己的价值；不卑不亢的去面试吧，你是被挑选的人，也是选择公司的人。 我的春招面试记录https://kongdada.github.io/archives/page/2/还有任何疑问可以去留言板给我留言。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[夜深人静，聊点生活]]></title>
    <url>%2F1896%2F12%2F15%2FlifeNum02%2F</url>
    <content type="text"><![CDATA[夜深人静，乱七八糟的开头像极了我平淡无奇的前二十四年人生。 乱七八糟的开头 2018年的冬天好像比以往的冬天要更加冷一些，人人都在说资本寒冬来临，人人都在说大环境不好。在肉眼可见的新闻里，企业的日子开始难过，纷纷开始裁员自保，以便顺利度过这个难受的冬天；也确实最近一些日子有很多公司开始裁员，不裁员的也开始停止了招人。恰巧我是一个处在试用期的小菜鸟，所以只好期待公司收益能好点。怎么看呢！关注一点互谅网公司的股票吧。从工作开始也开始关注了一点股票的走势，暂时没有购买的想法与实力，只是关注一下走势，未来也许会尝试一点点。 相比于前些天的惶恐不安，这两天就有点放肆了。因为转正申请通过了领导审批，按照道理来说会顺利转正啦（但人生哪有顺利的事情！）。只好持有一直以来的良好心态，尽人事听天命。 夜深人静的白日废话从周五看起来这个周末也是平淡无奇，规规矩矩，未来会被想不起来的两天，没有任何特别的味道在里面。但很奇怪的是这个周六的清晨，醒来居然依旧有点疲惫。刷刷手机中午混了一顿麻辣香锅。下午看起来是一个不错的游戏时间。通常酒足饭饱思淫欲，呸！不是。就是午饭吃的有点饱，困了。我最后的记忆是勇士领先国王十几分，看来又是一场不温不火的比赛。妈的，也不认真防守。还是很喜欢去年的大卫西撮着牙花子的样子，真是让人觉得力量感十足，还有点可爱。再醒来就是下午六点了，在屋子里踱了几个来回，算了不想出门煮个面条吃点算了。 夜深人静的开心时光最近迷上了一本小说，名字叫《将夜》。回顾过去的日子我很清楚的记得我几个痴迷小说的点；突然这么一联想还觉得挺有意思一件事。 第一本叫《挪威的森林》，令人着迷的村上先生。我高二抢了舍友的书来看，都以为是当小黄书来看，我向别人介绍这本书也是说作为小黄书来看的，但是我明白那不是真的。我明白那书里有我需要的孤独感，有我需要的放纵感。现实生活可以限制很多的行为，但思维可以在小说里肆无忌惮的乱冲，那一刻我就是男主人公，我更爱一个像绿子那样的女人。但我知道现实里大概不能有。 第二是一个系列叫《平凡的生活》，一共三本书。迷上他们的时候正好在高三，也是家庭走向另一个主基调的时刻，放弃种地，开始外出打工。当然了一个家庭的基调要变，哪有那么容易。总有一方希望守旧保持安稳，与此相对另一方希望有点不同，改变当前的困局。免不了的争吵发生，也免不了的情感冲突。正好此时来自黄土高原陕西人孙少平一家给与了我一个看客的身份。虽说2012年2013年生活总归比书中那个年代强，但从一个家庭的角度来讲，所要面对的困难却很类似。此时我也是男主人公，一个从全身心有了自己的想法，并且实践希望做一个家庭的小顶梁柱。当然这其实对于当时的我是一种伤害。因为早开的花，必然过早的衰败。 第三是一个种类，大学时期看的玄幻。《斗破苍穹》《我欲封天》《大主宰》…后来其实也算是看厌了这个类型。大学时期可能是生活比较安稳，情感也比较安稳，再也没有过那种痴迷某一本书，然后带来醍醐灌顶的感觉。 在我沉迷于某一本书的时候，我总会感觉到思维在另一个层次。这种感觉真的是太过于美好了。我总能在那个时期完成一些比较艰难的事情。比如在高二的课堂上解决了老师提出来的一道难题，比如在高三从家庭的琐碎中抽离出来，保持平静的复习。不知道以后会不会再有这种时刻，我很怀念那样的时刻。 其实现在看村上春树依旧很享受他带来的孤独与勇敢，只是再也做不到抽离出另一个精神世界，解决所有艰难的问题。我想写小说的人都是妖怪吧。 夜深人静的废话 看小说混合着打游戏，转眼来到了周六的最后半个小时。突然觉得该出去溜达溜达。磨磨蹭蹭，想说服某个舍友跟我出去走走，但是最后还是失败了。因为半夜三更无所事事的出去溜达一圈，怎么看都不太正常。 凌晨果然和舍友说的一样没有几家开着的店。但是24时的便利店，在这个地方居然有三家，其中两家还是邻居，剩下的一家跟他们隔着一条马路相互遥望。我在第一家店买了一包瓜子，还碰到一个正在买东西的外国小哥。来到北京后见到的外国小哥好像都喜欢味道比较重的香水。在第二家买了一包馍片和玉米肠，结账时他问我手里的瓜子是不是，我告诉他不是。穿过了马路来到了第三家，转了一圈买了一瓶茉莉蜜茶，没有看到我最爱的茉莉清茶。结账时他又问我手里的瓜子是不是，我说实在对面买的。说完我就在想，这个回答好像是一个解释，为什么不直接回答不是呢！ 回来的路上就吃掉了可爱的玉米香肠，好是好就是分量有点少。 夜深人静 这么一个精力充沛的夜晚，这么一个安静的夜晚，就不要辜负了吧，不如把纠结了快一个礼拜的小文章好好写完吧。所以我写下了这篇，也是博客第二篇属于聊生活的文章。 现在我的博客其实对于我自己更加重要，对于陌生的看客一点也不重要。每一篇技术文章，其实是我对自己做的事情的总结，在写文章的过程中，总结整个过程，理清思路就好。为什么开始写这样的生活类的小文章了呢！因为前些日子看了一个老博客主的总结，具体的话早忘了，高瞻远瞩的总结也忘了。但大概可能就是说博客只是一个用来记录思考过程的地方。想写什么就写点什么。因为生活里永远不止技术。]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从求职到入职后一个月的生活]]></title>
    <url>%2F1896%2F08%2F16%2FFuckLifeNum01%2F</url>
    <content type="text"><![CDATA[从求职倒入职一个月，很幸运，在我所有面试中自我感觉发挥的不算最好的公司发给我一个offer。发挥的最好的公司却拒绝了我。 求职总结除了踏踏实实学技术，写代码。总结来总结去就一句话，“求职要趁早”。所谓一个萝卜一个坑，公司招人也是有成本的，所以招人也没有精力去寻找绝对最优秀的人，而是要找到达标的人，所以努力达到那个标准就好。但假设有两个达标的人，目标公司的这个坑被先参加面试的人给占据了，那么后来者就连面试机会都没了，所以趁早参加求职。把握机会而已。 毕业季毕业季，激动了两次，一次是全场合唱校歌；不知道是激动还是不舍。我分不清其中的感情，到现在也没有把当时的自己理清楚。最有可能是对于学生时代真的结束的不舍得和一丝丝对未来的恐惧。另一次是6.29还是6.30，天！真的健忘。离开宿舍，被恕哥抱着不撒手给惹哭了。你很难想象两个大男人抱一起不撒手的场面，有点滑稽，但很感动。大学四年终究是留下了太多的遗憾。不知道以后要花多少时间来弥补。后来人请努力学习吧，该点的技能早点，一毕业都是白花花的银子估量你的价值。 入职7.4号顺利入职。努力开启新的生活。 人事变动入职一个月之际，正准备开启技术新篇章的时候被通知要换个小组。说实话，第一天很难接受。刚适应的环境，刚打破的人际关系的尴尬期。刚准备好开启新征程。却要被换小组，接受新的技术环境，甚至新的方向。调整了两天，想明白了。毕业基本等于一张白纸，最多自己画了点轮廓，但那么多的细节，先画那个都可以。况且java这个技能迟早都得点。 最近在做的事java spring springMVC mybatis]]></content>
      <categories>
        <category>Life</category>
      </categories>
      <tags>
        <tag>life</tag>
      </tags>
  </entry>
</search>
